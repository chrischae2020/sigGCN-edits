{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "sigGCN_running.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNENA4HfQl4N3dNFhwWh1yZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrischae2020/sigGCN-edits/blob/main/train_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r31cCABInETI"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1T-BeJ6g1V_GDtBtv2vucUKsAcfcCyYFf?authuser=1#scrollTo=j57beg1VlEks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmgdFkO7DsSk",
        "outputId": "0e45b66f-abd6-407a-9b3e-69094694232c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HR9Y4H67D11t",
        "outputId": "5c472904-ef0b-4979-9bcb-93176506f772"
      },
      "source": [
        "#changing the working directory\n",
        "%cd /content/gdrive/MyDrive/Github/sigGCN-main/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Github/sigGCN-main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uw1Te4mdqrG"
      },
      "source": [
        "# BaronHuman"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW8Twsg6dt87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d6b4340-7611-4f4c-cf8d-da3d8ee730cb"
      },
      "source": [
        "!python siggcn.py --dirData=\"data/\" --dataset=\"BaronHuman\" --dirAdj=\"data/BaronHuman/\" --dirLabel=\"data/BaronHuman/\" --outputDir=\"data/output\" --saveResults=0 --epochs=20 --batchsize=30 --savePlot=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda available\n",
            "cuda available\n",
            "load data...\n",
            "tcmalloc: large alloc 1199595520 bytes == 0x5617fdeb6000 @  0x7f05d24451e7 0x7f057614846e 0x7f0576198c7b 0x7f057619935f 0x7f057623b103 0x561780691544 0x561780691240 0x561780705627 0x561780692afa 0x561780700915 0x5617806ff9ee 0x561780692bda 0x561780700915 0x561780692afa 0x561780700915 0x561780692afa 0x561780700915 0x5617806ff9ee 0x561780692bda 0x561780701737 0x5617806ffced 0x561780692bda 0x561780701737 0x5617806ffced 0x56178069348c 0x5617806d4159 0x5617806d10a4 0x561780691d49 0x56178070594f 0x5617806ff9ee 0x561780692bda\n",
            "duplicated rows: []\n",
            "(8569, 17499)\n",
            "(17499, 17499)\n",
            "maxscale: 8.3707791729607\n",
            "load done.\n",
            "adj_shape: (1000, 1000)  [# cell, # gene] (8569, 1000)\n",
            "No existing network to delete\n",
            "\n",
            "num_epochs= 20 , train_size= 6855 , nb_iter= 4570\n",
            "Graph ConvNet: GCN\n",
            "--------- 1000\n",
            "nb of parameters= 20062 \n",
            "\n",
            "Graph_GCN(\n",
            "  (cl1): Linear(in_features=5, out_features=5, bias=True)\n",
            "  (fc1): Linear(in_features=625, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=1000, bias=True)\n",
            "  (fc3): Linear(in_features=1000, out_features=1000, bias=True)\n",
            "  (cnn_fc1): Linear(in_features=8192, out_features=32, bias=True)\n",
            "  (FC_concat): Linear(in_features=8817, out_features=14, bias=True)\n",
            "  (FC_sum2): Linear(in_features=64, out_features=14, bias=True)\n",
            "  (FC_sum1): Linear(in_features=64, out_features=14, bias=True)\n",
            "  (nn_fc1): Linear(in_features=1000, out_features=256, bias=True)\n",
            "  (nn_fc2): Linear(in_features=256, out_features=32, bias=True)\n",
            "  (nn_fc3): Linear(in_features=32, out_features=256, bias=True)\n",
            "  (nn_fc4): Linear(in_features=32, out_features=1000, bias=True)\n",
            ")\n",
            "epoch= 1, loss(train)= 0.940, accuracy(train)= 0.809, time= 9.533, lr= 0.01000\n",
            "training_time: 9.532785892486572\n",
            "epoch= 2, loss(train)= 0.409, accuracy(train)= 0.954, time= 9.570, lr= 0.00950\n",
            "training_time: 9.570197820663452\n",
            "epoch= 3, loss(train)= 0.338, accuracy(train)= 0.968, time= 9.412, lr= 0.00903\n",
            "training_time: 9.41183066368103\n",
            "epoch= 4, loss(train)= 0.304, accuracy(train)= 0.974, time= 9.512, lr= 0.00857\n",
            "training_time: 9.512061357498169\n",
            "epoch= 5, loss(train)= 0.285, accuracy(train)= 0.978, time= 9.394, lr= 0.00815\n",
            "training_time: 9.393505096435547\n",
            "epoch= 6, loss(train)= 0.269, accuracy(train)= 0.982, time= 9.523, lr= 0.00774\n",
            "training_time: 9.522678852081299\n",
            "epoch= 7, loss(train)= 0.260, accuracy(train)= 0.983, time= 9.502, lr= 0.00735\n",
            "training_time: 9.501573324203491\n",
            "epoch= 8, loss(train)= 0.251, accuracy(train)= 0.985, time= 9.301, lr= 0.00698\n",
            "training_time: 9.30066967010498\n",
            "epoch= 9, loss(train)= 0.244, accuracy(train)= 0.987, time= 9.307, lr= 0.00663\n",
            "training_time: 9.307463645935059\n",
            "epoch= 10, loss(train)= 0.239, accuracy(train)= 0.987, time= 9.385, lr= 0.00630\n",
            "training_time: 9.385226249694824\n",
            "epoch= 11, loss(train)= 0.234, accuracy(train)= 0.990, time= 9.331, lr= 0.00599\n",
            "----accuracy(val)=  0.9708284714119019\n",
            "training_time: 9.331172227859497\n",
            "epoch= 12, loss(train)= 0.229, accuracy(train)= 0.991, time= 9.237, lr= 0.00569\n",
            "training_time: 9.237304925918579\n",
            "epoch= 13, loss(train)= 0.225, accuracy(train)= 0.991, time= 9.350, lr= 0.00540\n",
            "training_time: 9.349892139434814\n",
            "epoch= 14, loss(train)= 0.222, accuracy(train)= 0.992, time= 9.306, lr= 0.00513\n",
            "training_time: 9.305850982666016\n",
            "epoch= 15, loss(train)= 0.219, accuracy(train)= 0.993, time= 9.177, lr= 0.00488\n",
            "training_time: 9.176633358001709\n",
            "epoch= 16, loss(train)= 0.217, accuracy(train)= 0.994, time= 9.090, lr= 0.00463\n",
            "training_time: 9.089966297149658\n",
            "epoch= 17, loss(train)= 0.215, accuracy(train)= 0.994, time= 9.432, lr= 0.00440\n",
            "training_time: 9.43159556388855\n",
            "epoch= 18, loss(train)= 0.213, accuracy(train)= 0.994, time= 9.215, lr= 0.00418\n",
            "training_time: 9.214945554733276\n",
            "epoch= 19, loss(train)= 0.211, accuracy(train)= 0.996, time= 9.209, lr= 0.00397\n",
            "training_time: 9.20911955833435\n",
            "epoch= 20, loss(train)= 0.209, accuracy(train)= 0.995, time= 9.275, lr= 0.00377\n",
            "training_time: 9.274973630905151\n",
            "  accuracy(val) = 0.979 %, time= 0.062\n",
            "  accuracy(test) = 0.975 %, time= 0.061\n",
            "n_classes:  14\n",
            "Saved plot as roc_auc_BaronHuman.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQZWNI3rdmKJ"
      },
      "source": [
        "# Zhengsorted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUClOFx1D6D9",
        "outputId": "b39a4341-deaa-4f35-8373-8836a3637f1c"
      },
      "source": [
        "!python siggcn.py --dirData=\"data/\" --dataset=\"Zhengsorted\" --dirAdj=\"data/Zhengsorted/\" --dirLabel=\"data/Zhengsorted/\" --outputDir=\"data/output\" --saveResults=0 --epochs=20 --savePlot=1\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda available\n",
            "cuda available\n",
            "load data...\n",
            "tcmalloc: large alloc 3512320000 bytes == 0x559bf2872000 @  0x7f8278d9d1e7 0x7f821caa046e 0x7f821caf0c7b 0x7f821caf135f 0x7f821cb93103 0x559ad1424544 0x559ad1424240 0x559ad1498627 0x559ad1425afa 0x559ad1493915 0x559ad14929ee 0x559ad1425bda 0x559ad1493915 0x559ad1425afa 0x559ad1493915 0x559ad1425afa 0x559ad1493915 0x559ad14929ee 0x559ad1425bda 0x559ad1494737 0x559ad1492ced 0x559ad1425bda 0x559ad1494737 0x559ad1492ced 0x559ad142648c 0x559ad1467159 0x559ad14640a4 0x559ad1424d49 0x559ad149894f 0x559ad14929ee 0x559ad1425bda\n",
            "duplicated rows: [2911 7515]\n",
            "tcmalloc: large alloc 3512000512 bytes == 0x559cc3e0e000 @  0x7f8278d9d1e7 0x7f821caa046e 0x7f821caf0c7b 0x7f821caf135f 0x7f821cb93103 0x559ad1424544 0x559ad1424240 0x559ad1498627 0x559ad14929ee 0x559ad1425bda 0x559ad1494737 0x559ad14929ee 0x559ad1425bda 0x559ad1494737 0x559ad1492ced 0x559ad1425bda 0x559ad1493915 0x559ad1492ced 0x559ad1425bda 0x559ad1494737 0x559ad14929ee 0x559ad1425bda 0x559ad1494737 0x559ad14929ee 0x559ad1425bda 0x559ad1494737 0x559ad14929ee 0x559ad1425bda 0x559ad1494737 0x559ad14929ee 0x559ad1425bda\n",
            "(20000, 21950)\n",
            "(21950, 21950)\n",
            "tcmalloc: large alloc 3512000512 bytes == 0x559d95b66000 @  0x7f8278d9d1e7 0x7f821caa046e 0x7f821caf0c7b 0x7f821caf0d18 0x7f821cbacd79 0x7f821cbafe4c 0x7f821cccee7f 0x7f821ccd4fb5 0x7f821ccd6e3d 0x7f821ccd8516 0x559ad1424c52 0x559ad14984d9 0x559ad1425afa 0x559ad1497d00 0x559ad14929ee 0x559ad14926f3 0x559ad155c4c2 0x559ad155c83d 0x559ad155c6e6 0x559ad1534163 0x559ad1533e0c 0x7f8277b87bf7 0x559ad1533cea\n",
            "maxscale: 5.517452896464707\n",
            "tcmalloc: large alloc 3512000512 bytes == 0x559e670b4000 @  0x7f8278d9d1e7 0x7f821caa046e 0x7f821caf0c7b 0x7f821caf0d18 0x7f821ccd540f 0x7f821ccd6e3d 0x7f821ccd8516 0x559ad1425720 0x559ad14252f9 0x7f821cbb74d8 0x559ad1407933 0x559ad150e496 0x559ad149584a 0x559ad1425afa 0x559ad1497d00 0x559ad14929ee 0x559ad14926f3 0x559ad155c4c2 0x559ad155c83d 0x559ad155c6e6 0x559ad1534163 0x559ad1533e0c 0x7f8277b87bf7 0x559ad1533cea\n",
            "tcmalloc: large alloc 3512000512 bytes == 0x559d95b66000 @  0x7f8278d9d1e7 0x7f821caa046e 0x7f821caf0c7b 0x7f821caf0d18 0x7f821cbacd79 0x7f821cbafe4c 0x7f821cccee7f 0x7f821ccd4fb5 0x7f821ccd6e3d 0x7f821ccd8516 0x559ad1425720 0x559ad14252f9 0x7f821cbb70db 0x559ad150e0b2 0x559ad149462d 0x559ad14929ee 0x559ad1364e2b 0x559ad1494fe4 0x559ad14929ee 0x559ad1364e2b 0x7f821caddef7 0x559ad1424437 0x559ad1424240 0x559ad1497973 0x559ad14929ee 0x559ad1425bda 0x559ad1494737 0x559ad14929ee 0x559ad1425bda 0x559ad1494737 0x559ad1425afa\n",
            "load done.\n",
            "adj_shape: (1000, 1000)  [# cell, # gene] (20000, 1000)\n",
            "No existing network to delete\n",
            "\n",
            "num_epochs= 20 , train_size= 16000 , nb_iter= 5000\n",
            "Graph ConvNet: GCN\n",
            "--------- 1000\n",
            "nb of parameters= 20062 \n",
            "\n",
            "Graph_GCN(\n",
            "  (cl1): Linear(in_features=5, out_features=5, bias=True)\n",
            "  (fc1): Linear(in_features=625, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=1000, bias=True)\n",
            "  (fc3): Linear(in_features=1000, out_features=1000, bias=True)\n",
            "  (cnn_fc1): Linear(in_features=8192, out_features=32, bias=True)\n",
            "  (FC_concat): Linear(in_features=8817, out_features=10, bias=True)\n",
            "  (FC_sum2): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (FC_sum1): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (nn_fc1): Linear(in_features=1000, out_features=256, bias=True)\n",
            "  (nn_fc2): Linear(in_features=256, out_features=32, bias=True)\n",
            "  (nn_fc3): Linear(in_features=32, out_features=256, bias=True)\n",
            "  (nn_fc4): Linear(in_features=32, out_features=1000, bias=True)\n",
            ")\n",
            "epoch= 1, loss(train)= 1.587, accuracy(train)= 0.570, time= 10.484, lr= 0.01000\n",
            "training_time: 10.484417915344238\n",
            "epoch= 2, loss(train)= 0.729, accuracy(train)= 0.806, time= 10.250, lr= 0.00950\n",
            "training_time: 10.24951457977295\n",
            "epoch= 3, loss(train)= 0.578, accuracy(train)= 0.852, time= 10.352, lr= 0.00903\n",
            "training_time: 10.352447509765625\n",
            "epoch= 4, loss(train)= 0.512, accuracy(train)= 0.879, time= 10.573, lr= 0.00857\n",
            "training_time: 10.572803497314453\n",
            "epoch= 5, loss(train)= 0.470, accuracy(train)= 0.897, time= 10.351, lr= 0.00815\n",
            "training_time: 10.35055685043335\n",
            "epoch= 6, loss(train)= 0.450, accuracy(train)= 0.904, time= 10.389, lr= 0.00774\n",
            "training_time: 10.38915467262268\n",
            "epoch= 7, loss(train)= 0.427, accuracy(train)= 0.912, time= 10.433, lr= 0.00735\n",
            "training_time: 10.432594776153564\n",
            "epoch= 8, loss(train)= 0.411, accuracy(train)= 0.919, time= 10.351, lr= 0.00698\n",
            "training_time: 10.350935220718384\n",
            "epoch= 9, loss(train)= 0.398, accuracy(train)= 0.925, time= 10.352, lr= 0.00663\n",
            "training_time: 10.352129459381104\n",
            "epoch= 10, loss(train)= 0.388, accuracy(train)= 0.928, time= 10.378, lr= 0.00630\n",
            "training_time: 10.377528429031372\n",
            "epoch= 11, loss(train)= 0.380, accuracy(train)= 0.929, time= 10.348, lr= 0.00599\n",
            "----accuracy(val)=  0.8725\n",
            "training_time: 10.348015069961548\n",
            "epoch= 12, loss(train)= 0.379, accuracy(train)= 0.929, time= 10.394, lr= 0.00569\n",
            "training_time: 10.394020795822144\n",
            "epoch= 13, loss(train)= 0.365, accuracy(train)= 0.935, time= 10.614, lr= 0.00540\n",
            "training_time: 10.61392855644226\n",
            "epoch= 14, loss(train)= 0.364, accuracy(train)= 0.936, time= 10.522, lr= 0.00513\n",
            "training_time: 10.52205228805542\n",
            "epoch= 15, loss(train)= 0.356, accuracy(train)= 0.939, time= 10.549, lr= 0.00488\n",
            "training_time: 10.549140214920044\n",
            "epoch= 16, loss(train)= 0.352, accuracy(train)= 0.939, time= 10.381, lr= 0.00463\n",
            "training_time: 10.38123345375061\n",
            "epoch= 17, loss(train)= 0.344, accuracy(train)= 0.944, time= 10.414, lr= 0.00440\n",
            "training_time: 10.414032697677612\n",
            "epoch= 18, loss(train)= 0.339, accuracy(train)= 0.946, time= 10.363, lr= 0.00418\n",
            "training_time: 10.363432168960571\n",
            "epoch= 19, loss(train)= 0.340, accuracy(train)= 0.943, time= 10.441, lr= 0.00397\n",
            "training_time: 10.44093108177185\n",
            "epoch= 20, loss(train)= 0.330, accuracy(train)= 0.949, time= 10.184, lr= 0.00377\n",
            "training_time: 10.18405532836914\n",
            "  accuracy(val) = 0.911 %, time= 0.111\n",
            "  accuracy(test) = 0.910 %, time= 0.109\n",
            "n_classes:  10\n",
            "Saved plot as roc_auc_Zhengsorted.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS5paEozY-ei"
      },
      "source": [
        "# Writing python files\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CKDLVqcD14r"
      },
      "source": [
        "%pycat train.py\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z4I1Rq4CvrA",
        "outputId": "8f4e780f-7869-421d-e07d-0f2d1a6ab6d9"
      },
      "source": [
        "%%writefile train.py\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Mar 23 18:30:20 2021\n",
        "\n",
        "@author: tianyu\n",
        "\"\"\"\n",
        "import time\n",
        "import pandas as pd\n",
        "import torch\n",
        "#from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn import metrics, preprocessing\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "import numpy as np\n",
        "import sys\n",
        "sys.path.insert(0, 'lib/')\n",
        "import utilsdata\n",
        "\n",
        "def calc_auc(test_labels, preds_probs, dataset):\n",
        "  if (dataset == 'Zhengsorted'):\n",
        "    classes = [0,1,2,3,4,5,6,7,8,9]\n",
        "  elif (dataset == 'BaronHuman'):\n",
        "    classes = [0,1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
        "  \n",
        "  test_labels = preprocessing.label_binarize(test_labels.values, classes=classes)\n",
        "\n",
        "  n_classes = test_labels.shape[1]\n",
        "  print('n_classes: ', n_classes)\n",
        "\n",
        "  fpr = dict()\n",
        "  tpr = dict()\n",
        "  roc_auc = dict()\n",
        "  for i in range(n_classes):\n",
        "      fpr[i], tpr[i], _ = roc_curve(test_labels[:, i], preds_probs[:, i])\n",
        "      roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "  fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(test_labels.ravel(), preds_probs.ravel())\n",
        "  roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "  return fpr, tpr, roc_auc\n",
        "\n",
        "\n",
        "def calculation(pred_test, test_labels, preds_probs, dataset, method='GCN'):\n",
        "    test_acc = metrics.accuracy_score(pred_test, test_labels)\n",
        "    test_f1_macro = metrics.f1_score(pred_test, test_labels, average='macro')\n",
        "    test_f1_micro = metrics.f1_score(pred_test, test_labels, average='micro')\n",
        "    precision = metrics.precision_score(test_labels, pred_test, average='micro')\n",
        "    recall = metrics.recall_score(test_labels, pred_test, average='micro')\n",
        "    fpr, tpr, roc_auc = calc_auc(test_labels, preds_probs, dataset)\n",
        "\n",
        "    # print('method | ','test_acc | ','f1_test_macro | ','f1_test_micro | ', 'test precision | ', 'test recall')\n",
        "    # print(method, \" | \", test_acc, \" | \", test_f1_macro, \" | \", test_f1_micro, \" | \", precision, \" | \", recall)\n",
        "\n",
        "    return fpr, tpr, roc_auc\n",
        "        \n",
        "def weight_init(m):\n",
        "    if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None: \n",
        "            m.bias.data.fill_(0.0)\n",
        "\n",
        "def test_model(net, loader, L, args):\n",
        "    t_start_test = time.time()\n",
        "    \n",
        "    net.eval()\n",
        "    test_acc = 0\n",
        "    count = 0\n",
        "    confusionGCN = np.zeros([args.nclass, args.nclass])\n",
        "    predictions = pd.DataFrame()\n",
        "    y_true = []\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "    \n",
        "#    for batch_x, batch_y in loader:\n",
        "#        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "#\n",
        "#        out_gae, out_hidden, pred, out_adj = net(batch_x, args.dropout, L)\n",
        "#        \n",
        "#        test_acc += utilsdata.accuracy(pred, batch_y).item()\n",
        "#        count += 1\n",
        "#        y_true.append(batch_y.item())\n",
        "#        #y_pred.append(pred.max(1)[1].item())\n",
        "#        confusionGCN[batch_y.item(), pred.max(1)[1].item()] += 1\n",
        "#        px = pd.DataFrame(pred.detach().cpu().numpy())            \n",
        "#        predictions = pd.concat((predictions, px),0)\n",
        "#        \n",
        "#    t_total_test = time.time() - t_start_test\n",
        "#    preds_labels = np.argmax(np.asarray(predictions), 1)\n",
        "#    test_acc = test_acc/count\n",
        "#    predictions.insert(0, 'trueLabels', y_true)\n",
        "\n",
        "\n",
        "    for batch_x, batch_y in loader:\n",
        "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "        out_gae, out_hidden, pred, out_adj = net(batch_x, args.dropout, L)\n",
        "        \n",
        "        test_acc += utilsdata.accuracy(pred, batch_y).item() * len(batch_y)\n",
        "        count += 1\n",
        "        y_true = batch_y.detach().cpu().numpy()\n",
        "        y_predProbs = pred.detach().cpu().numpy()\n",
        "        \n",
        "    predictions = pd.DataFrame(y_predProbs)            \n",
        "    for i in range(len(y_true)):\n",
        "        confusionGCN[y_true[i], np.argmax(y_predProbs[i,:])] += 1\n",
        "    \n",
        "    t_total_test = time.time() - t_start_test\n",
        "    preds_labels = np.argmax(np.asarray(predictions), 1)\n",
        "    preds_probs = np.asarray(predictions)\n",
        "    test_acc = test_acc/len(loader.dataset)\n",
        "    predictions.insert(0, 'trueLabels', y_true)\n",
        "    n_classes = preds_probs.shape[1]\n",
        "    \n",
        "    return test_acc, confusionGCN, predictions, preds_labels, t_total_test, preds_probs, n_classes\n",
        "\n",
        "        \n",
        "def train_model(useModel, train_loader, val_loader, L, args):    \n",
        "\n",
        "    # network parameters\n",
        "    D_g = args.num_gene\n",
        "    CL1_F = 5\n",
        "    CL1_K = 5\n",
        "    FC1_F = 32\n",
        "    FC2_F = 0\n",
        "    NN_FC1 = 256\n",
        "    NN_FC2 = 32\n",
        "    out_dim = args.nclass  \n",
        "    net_parameters = [D_g, CL1_F, CL1_K, FC1_F,FC2_F,NN_FC1, NN_FC2, out_dim]\n",
        "\n",
        "    # learning parameters\n",
        "    dropout_value = 0.2\n",
        "    l2_regularization = 5e-4\n",
        "    batch_size = args.batchsize\n",
        "    num_epochs = args.epochs\n",
        "    \n",
        "\n",
        "    nb_iter = int(num_epochs * args.train_size) // batch_size\n",
        "    print('num_epochs=',num_epochs,', train_size=',args.train_size,', nb_iter=',nb_iter)\n",
        "    \n",
        "    # Optimizer\n",
        "    global_lr = args.lr\n",
        "    global_step = 0\n",
        "    decay = 0.95\n",
        "    decay_steps = args.train_size\n",
        "        \n",
        "        \n",
        "   # instantiate the object net of the class\n",
        "    net = useModel(net_parameters)\n",
        "    net.apply(weight_init)\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        net.cuda()\n",
        "        \n",
        "    print(net)\n",
        "            \n",
        "    #optimizer = optim.Adam(net.parameters(),lr= args.lr, weight_decay=5e-4)\n",
        "    optimizer = optim.SGD(net.parameters(), momentum=0.9, lr= args.lr)\n",
        "    \n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "    \n",
        "    ## Train   \n",
        "    net.train()\n",
        "    losses_train = []\n",
        "    acc_train = []\n",
        "    \n",
        "    t_total_train = time.time()\n",
        "\n",
        "    def adjust_learning_rate(optimizer, epoch, lr):\n",
        "        \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    #    lr = args.lr * (0.1 ** (epoch // 30))\n",
        "        lr = lr * pow( decay , float(global_step// decay_steps) )\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        return lr\n",
        "    \n",
        "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "    \n",
        "        # update learning rate\n",
        "        cur_lr = adjust_learning_rate(optimizer,epoch, args.lr)\n",
        "        \n",
        "        # reset time\n",
        "        t_start = time.time()\n",
        "    \n",
        "        # extract batches\n",
        "        epoch_loss = 0.0\n",
        "        epoch_acc = 0.0\n",
        "        count = 0\n",
        "        for i, (batch_x, batch_y) in enumerate(train_loader):\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "                    \n",
        "            optimizer.zero_grad()   \n",
        "            out_gae, out_hidden, output, out_adj = net(batch_x, dropout_value, L)\n",
        "\n",
        "            loss_batch = net.loss(out_gae, batch_x, output, batch_y, l2_regularization)\n",
        "          \n",
        "            acc_batch = utilsdata.accuracy(output, batch_y).item()\n",
        "            \n",
        "            loss_batch.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            count += 1\n",
        "            epoch_loss += loss_batch.item()\n",
        "            epoch_acc += acc_batch\n",
        "            global_step += args.batchsize \n",
        "            \n",
        "            # print\n",
        "            if count % 1000 == 0: # print every x mini-batches\n",
        "                print('epoch= %d, i= %4d, loss(batch)= %.4f, accuray(batch)= %.2f' % (epoch + 1, count, loss_batch.item(), acc_batch))\n",
        "    \n",
        "    \n",
        "        epoch_loss /= count\n",
        "        epoch_acc /= count\n",
        "        losses_train.append(epoch_loss) # Calculating the loss\n",
        "        acc_train.append(epoch_acc) # Calculating the acc\n",
        "        # print\n",
        "        t_stop = time.time() - t_start\n",
        "        \n",
        "        if epoch % 10 == 0 and epoch != 0:\n",
        "            with torch.no_grad():\n",
        "                val_acc = 0  \n",
        "                count = 0\n",
        "                for b_x, b_y in val_loader:\n",
        "                    b_x, b_y = b_x.to(device), b_y.to(device)          \n",
        "                    _, _, val_pred, _ = net(b_x, args.dropout, L)                    \n",
        "                    val_acc += utilsdata.accuracy(val_pred, b_y).item() * len(b_y)\n",
        "                    count += 1\n",
        "                    \n",
        "                val_acc = val_acc/len(val_loader.dataset)\n",
        "                \n",
        "            print('epoch= %d, loss(train)= %.3f, accuracy(train)= %.3f, time= %.3f, lr= %.5f' %\n",
        "                  (epoch + 1, epoch_loss, epoch_acc, t_stop, cur_lr))\n",
        "            print('----accuracy(val)= ', val_acc)\n",
        "            print('training_time:',t_stop)\n",
        "        else:\n",
        "            print('epoch= %d, loss(train)= %.3f, accuracy(train)= %.3f, time= %.3f, lr= %.5f' %\n",
        "                  (epoch + 1, epoch_loss, epoch_acc, t_stop, cur_lr))\n",
        "            print('training_time:',t_stop)\n",
        "        \n",
        "    \n",
        "    t_total_train = time.time() - t_total_train  \n",
        "    \n",
        "    return net, t_total_train\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctZe1dRuD17l",
        "outputId": "954c7f24-1aea-486a-e2fe-6abd9c02436c"
      },
      "source": [
        "%%writefile siggcn.py\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Mar 22 14:00:37 2020\n",
        "\n",
        "@author: tianyu\n",
        "\"\"\"\n",
        "   \n",
        "import sys, os\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as Data\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import scipy.sparse as sp\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "import pandas as pd\n",
        "import sys\n",
        "sys.path.insert(0, 'lib/')\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print('cuda available')\n",
        "    dtypeFloat = torch.cuda.FloatTensor\n",
        "    dtypeLong = torch.cuda.LongTensor\n",
        "    torch.cuda.manual_seed(1)\n",
        "else:\n",
        "    print('cuda not available')\n",
        "    dtypeFloat = torch.FloatTensor\n",
        "    dtypeLong = torch.LongTensor\n",
        "    torch.manual_seed(1)\n",
        "\n",
        "from coarsening import coarsen, laplacian\n",
        "from coarsening import lmax_L\n",
        "from coarsening import perm_data\n",
        "from coarsening import rescale_L\n",
        "from layermodel import *\n",
        "import utilsdata\n",
        "from utilsdata import *\n",
        "from train import *\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "#\n",
        "#\n",
        "# Directories.\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--dirData', type=str, default='/Users/tianyu/Desktop/scRNAseq_Benchmark_datasets/Intra-dataset/', help=\"directory of cell x gene matrix\")\n",
        "parser.add_argument('--dataset', type=str, default='Zhengsorted', help=\"dataset\")\n",
        "parser.add_argument('--dirAdj', type = str, default = '/Users/tianyu/Desktop/scRNAseq_Benchmark_datasets/Intra-dataset/Zhengsorted/', help = 'directory of adj matrix')\n",
        "parser.add_argument('--dirLabel', type = str, default = '/Users/tianyu/Desktop/scRNAseq_Benchmark_datasets/Intra-dataset/Zhengsorted/', help = 'directory of adj matrix')\n",
        "parser.add_argument('--outputDir', type = str, default = 'data/output', help = 'directory to save results')\n",
        "parser.add_argument('--saveResults', type=int, default = 0, help='whether or not save the results')\n",
        "\n",
        "parser.add_argument('--normalized_laplacian', type=bool, default = True, help='Graph Laplacian: normalized.')\n",
        "parser.add_argument('--lr', type=float, default = 0.01, help='learning rate.')\n",
        "parser.add_argument('--num_gene', type=int, default = 1000, help='# of genes')\n",
        "parser.add_argument('--epochs', type=int, default = 1, help='# of epoch')\n",
        "parser.add_argument('--batchsize', type=int, default = 64, help='# of genes')\n",
        "parser.add_argument('--dropout', type=float, default = 0.2, help='dropout value')\n",
        "parser.add_argument('--id1', type=str, default = '', help='test in pancreas')\n",
        "parser.add_argument('--id2', type=str, default = '', help='test in pancreas')\n",
        "\n",
        "parser.add_argument('--net', type=str, default='String', help=\"netWork\")\n",
        "parser.add_argument('--dist', type=str, default='', help=\"dist type\")\n",
        "parser.add_argument('--sampling_rate', type=float, default = 1, help='# sampling rate of cells')\n",
        "\n",
        "parser.add_argument('--savePlot', type=int, default = 0, help='save resulting plot')\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "t_start = time.process_time()\n",
        "\n",
        "\n",
        "# Load data\n",
        "print('load data...')    \n",
        "adjall, alldata, labels, shuffle_index = utilsdata.load_largesc(path = args.dirData, dirAdj=args.dirAdj, dataset=args.dataset, net='String')\n",
        "\n",
        "# generate a fixed shuffle index\n",
        "if shuffle_index is not None:\n",
        "    shuffle_index = shuffle_index.astype(np.int32)\n",
        "else:\n",
        "    shuffle_index = np.random.permutation(alldata.shape[0])\n",
        "    np.savetxt(args.dirData +'/' + args.dataset +'/shuffle_index_'+args.dataset+'.txt')\n",
        "    \n",
        "train_all_data, adj = utilsdata.down_genes(alldata, adjall, args.num_gene)\n",
        "L = [laplacian(adj, normalized=True)]\n",
        "\n",
        "\n",
        "#####################################################\n",
        "\n",
        "##Split the dataset into train, val, test dataset. Use a fixed shuffle index to fix the sample order for comparison.\n",
        "train_data, val_data, test_data, train_labels, val_labels, test_labels = utilsdata.spilt_dataset(train_all_data, labels, shuffle_index)\n",
        "args.nclass = len(np.unique(labels))\n",
        "args.train_size = train_data.shape[0] \n",
        "\n",
        "## Use the train_data, val_data, test_data to generate the train, val, test loader\n",
        "train_loader, val_loader, test_loader = utilsdata.generate_loader(train_data,val_data, test_data, \n",
        "                                                        train_labels, val_labels, test_labels, \n",
        "                                                        args.batchsize)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##Delete existing network if exists\n",
        "try:\n",
        "    del net\n",
        "    print('Delete existing network\\n')\n",
        "except NameError:\n",
        "    print('No existing network to delete\\n')\n",
        "\n",
        "# Train model\n",
        "net, t_total_train = train_model(Graph_GCN, train_loader,val_loader, L, args)\n",
        "\n",
        "## Val\n",
        "val_acc,confusionGCN, predictions, preds_labels, t_total_test, preds_probs, n_classes = test_model(net, val_loader, L, args)\n",
        "print('  accuracy(val) = %.3f %%, time= %.3f' % (val_acc, t_total_test))\n",
        "\n",
        "# Test\n",
        "test_acc,confusionGCN, predictions, preds_labels, t_total_test, preds_probs, n_classes = test_model(net, test_loader, L, args)\n",
        "    \n",
        "print('  accuracy(test) = %.3f %%, time= %.3f' % (test_acc, t_total_test))\n",
        "fpr, tpr, roc_auc = calculation(preds_labels, predictions.iloc[:,0], preds_probs, args.dataset)\n",
        "# calculation(preds_labels, predictions.iloc[:,0], preds_probs)\n",
        "\n",
        "if args.saveResults:\n",
        "    testPreds4save = pd.DataFrame(preds_labels,columns=['predLabels'])\n",
        "    testPreds4save.insert(0, 'trueLabels', list(predictions.iloc[:,0]))\n",
        "    confusionGCN = pd.DataFrame(confusionGCN)\n",
        "    \n",
        "    testPreds4save.to_csv(args.outputDir+'/gcn_test_preds_'+ args.dataset+ str(args.num_gene)+'.csv')\n",
        "    predictions.to_csv(args.outputDir+'/gcn_testProbs_preds_'+ args.dataset+ str(args.num_gene) +'.csv')\n",
        "    confusionGCN.to_csv(args.outputDir+'/gcn_confuMat_'+ args.dataset+ str(args.num_gene)+'.csv')    \n",
        "    np.savetxt(args.outputDir+'/newgcn_train_time_'+args.dataset + str(args.num_gene) +'.txt', [t_total_train])   \n",
        "    np.savetxt(args.outputDir+'/newgcn_test_time_'+args.dataset + str(args.num_gene) +'.txt', [t_total_test])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from matplotlib.font_manager import FontProperties\n",
        "\n",
        "if args.savePlot == 1:\n",
        "    fontP = FontProperties()\n",
        "    fontP.set_size('x-small')\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "            label='micro-average ROC curve (area = {0:0.3f})'\n",
        "                  ''.format(roc_auc[\"micro\"]),\n",
        "            color='r', linestyle=':', linewidth=2)\n",
        "\n",
        "    colors = itertools.cycle(['aqua', 'fuchsia', 'darkorange', 'indigo', 'yellow', 'darkgreen',\n",
        "                              'teal', 'lime', 'brown', 'cadetblue', 'darkred', 'slategrey', 'purple', 'olive', \n",
        "                              'chocolate', 'crimson', 'lawngreen'])\n",
        "\n",
        "    if (args.dataset == 'Zhengsorted'):\n",
        "      celltypes = ['CD14+ Monocyte', 'CD19+ B', 'CD34+ (Pos_label)', 'CD4+ T Helper2', 'CD4+/CD25 T Reg',\n",
        "              'CD4+/CD45RA+/CD25- Naive T', 'CD4+/CD45RO+ Memory', 'CD56+ NK', 'CD8+ Cytotoxic T',\n",
        "              'CD8+/CD45RA+ Naive Cytotoxic']\n",
        "    elif (args.dataset == 'BaronHuman'):\n",
        "      celltypes = ['acinar', 'activated_stellate', 'alpha', 'beta', 'delta', 'ductal', 'endothelial', 'epsilon',\n",
        "                  'gamma', 'macrophage', 'mast', 'quiescent_stellate', 'schwann', 't_cell']\n",
        "\n",
        "    for i, color, cell in zip(range(n_classes), colors, celltypes):\n",
        "        lw = 2\n",
        "        plt.plot(fpr[i], tpr[i], color=color,\n",
        "                lw=lw, label='{0} (area {1:0.3f})'\n",
        "                .format(cell, roc_auc[i]))\n",
        "    # plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic')\n",
        "    plt.legend(loc=\"lower right\", prop=fontP)\n",
        "    plt.savefig('roc_auc_{}.png'.format(args.dataset))\n",
        "\n",
        "    print('Saved AUCROC plot as roc_auc_{}.png'.format(args.dataset))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting siggcn.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Efe6dQb06HnU"
      },
      "source": [
        "%%writefile lib/layermodel.py\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Jan 13 21:15:43 2020\n",
        "\n",
        "@author: tianyu\n",
        "\"\"\"\n",
        "import torch\n",
        "#from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import sys\n",
        "sys.path.insert(0, 'lib/')\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print('cuda available')\n",
        "    dtypeFloat = torch.cuda.FloatTensor\n",
        "    dtypeLong = torch.cuda.LongTensor\n",
        "    torch.cuda.manual_seed(1)\n",
        "else:\n",
        "    print('cuda not available')\n",
        "    dtypeFloat = torch.FloatTensor\n",
        "    dtypeLong = torch.LongTensor\n",
        "    torch.manual_seed(1)\n",
        "\n",
        "from coarsening import lmax_L\n",
        "from coarsening import rescale_L\n",
        "from utilsdata import sparse_mx_to_torch_sparse_tensor\n",
        "\n",
        "class my_sparse_mm(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Implementation of a new autograd function for sparse variables,\n",
        "    called \"my_sparse_mm\", by subclassing torch.autograd.Function\n",
        "    and implementing the forward and backward passes.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, W, x):  # W is SPARSE\n",
        "        ctx.save_for_backward(W, x)\n",
        "        y = torch.mm(W, x)\n",
        "        return y\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        W, x = ctx.saved_tensors\n",
        "        grad_input = grad_output.clone()\n",
        "        grad_input_dL_dW = torch.mm(grad_input, x.t())\n",
        "        grad_input_dL_dx = torch.mm(W.t(), grad_input )\n",
        "        return grad_input_dL_dW, grad_input_dL_dx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#########################################################################################################\n",
        "class Graph_GCN(nn.Module):\n",
        "\n",
        "    def __init__(self, net_parameters):\n",
        "\n",
        "        print('Graph ConvNet: GCN')\n",
        "\n",
        "        super(Graph_GCN, self).__init__()\n",
        "\n",
        "        # parameters\n",
        "        D_g, CL1_F, CL1_K,  FC1_F, FC2_F,NN_FC1, NN_FC2, out_dim = net_parameters\n",
        "        CNN1_F, CNN1_K = 32, 5\n",
        "        CL2_F, CL2_K = 10, 10\n",
        "        D_nn = D_g\n",
        "        self.in_dim = D_g\n",
        "        self.out_dim = out_dim\n",
        "        self.FC2_F = FC2_F\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.num_gene = D_nn\n",
        "        self.initScale = initScale = 6\n",
        "        self.poolsize = 8\n",
        "        FC1Fin = CL1_F*(D_g//self.poolsize)\n",
        "        self.FC1Fin = FC1Fin\n",
        "        self.CL1_K = CL1_K; self.CL1_F = CL1_F; \n",
        "        \n",
        "        # Feature_H, Feature_W = (Input_Height - filter_H + 2P)/S + 1, (Input_Width - filter_W + 2P)/S + 1\n",
        "        height = int(np.ceil(np.sqrt(int(D_nn))))\n",
        "        FC2Fin = int(CNN1_F * (height//2) ** 2)\n",
        "        self.FC2Fin = FC2Fin;\n",
        "        \n",
        "        # graph CL1\n",
        "        self.cl1 = nn.Linear(CL1_K, CL1_F)\n",
        "#        # graph CL2\n",
        "#        self.cl2 = nn.Linear(CL2_K*CL1_F, CL2_F)\n",
        "#        #FC gcnpure\n",
        "#        self.fc_gcnpure = nn.Linear(FC1Fin, self.out_dim)\n",
        "        # FC 1\n",
        "        self.fc1 = nn.Linear(FC1Fin, FC1_F)\n",
        "        # FC 2\n",
        "        if self.FC2_F == 0:\n",
        "            FC2_F = self.num_gene\n",
        "            print('---------',FC2_F)\n",
        "        self.fc2 = nn.Linear(FC1_F, FC2_F)\n",
        "        # FC 3\n",
        "        self.fc3 = nn.Linear(FC2_F, D_g)\n",
        "        # CNN_FC1\n",
        "        self.cnn_fc1 = nn.Linear(FC2Fin, FC1_F)\n",
        "        #FC_concat with CNN\n",
        "        Fin = FC1Fin + FC2Fin; Fout = self.out_dim;\n",
        "        self.FC_concat = nn.Linear(Fin, self.out_dim)             \n",
        "        #FC_sum2 with NN\n",
        "        Fin = FC1_F + NN_FC2; Fout = self.out_dim;\n",
        "        self.FC_sum2 = nn.Linear(Fin, Fout)                  \n",
        "        #FC_sum1 with CNN\n",
        "        Fin = FC1_F + FC1_F; Fout = self.out_dim;\n",
        "        self.FC_sum1 = nn.Linear(Fin, Fout)             \n",
        "        # NN_FC1\n",
        "        self.nn_fc1 = nn.Linear(self.in_dim, NN_FC1)\n",
        "        # NN_FC2\n",
        "        self.nn_fc2 = nn.Linear(NN_FC1, NN_FC2)\n",
        "        # NN_FC3_decode\n",
        "        self.nn_fc3 = nn.Linear(NN_FC2, NN_FC1)\n",
        "        # NN_FC4_decode\n",
        "        Fin = NN_FC2; Fout = self.in_dim;\n",
        "        self.nn_fc4 = nn.Linear(Fin, Fout)        \n",
        "\n",
        "        \n",
        "        # nb of parameters\n",
        "        nb_param = CL1_K* CL1_F + CL1_F          # CL1\n",
        "#        nb_param += CL2_K* CL1_F* CL2_F + CL2_F  # CL2\n",
        "        nb_param += FC1Fin* FC1_F + FC1_F        # FC1\n",
        "#        nb_param += FC1_F* FC2_F + FC2_F         # FC2\n",
        "        print('nb of parameters=',nb_param,'\\n')\n",
        "\n",
        "\n",
        "    def init_weights(self, W, Fin, Fout):\n",
        "\n",
        "        scale = np.sqrt( self.initScale / (Fin+Fout) )\n",
        "        W.uniform_(-scale, scale)\n",
        "        \n",
        "        return W\n",
        "\n",
        "\n",
        "    def graph_conv_cheby(self, x, cl,L, Fout, K):\n",
        "\n",
        "        # parameters\n",
        "        # B = batch size\n",
        "        # V = nb vertices\n",
        "        # Fin = nb input features\n",
        "        # Fout = nb output features\n",
        "        # K = Chebyshev order & support size\n",
        "        B, V, Fin = x.size(); B, V, Fin = int(B), int(V), int(Fin)\n",
        "\n",
        "        # rescale Laplacian\n",
        "        lmax = lmax_L(L)\n",
        "        L = rescale_L(L, lmax)\n",
        "\n",
        "        # convert scipy sparse matric L to pytorch\n",
        "        L = sparse_mx_to_torch_sparse_tensor(L)\n",
        "        if torch.cuda.is_available():\n",
        "            L = L.cuda()\n",
        "\n",
        "        # transform to Chebyshev basis\n",
        "        x0 = x.permute(1,2,0).contiguous()  # V x Fin x B\n",
        "        x0 = x0.view([V, Fin*B])            # V x Fin*B\n",
        "        x = x0.unsqueeze(0)                 # 1 x V x Fin*B\n",
        "\n",
        "        def concat(x, x_):\n",
        "            x_ = x_.unsqueeze(0)            # 1 x V x Fin*B\n",
        "            return torch.cat((x, x_), 0)    # K x V x Fin*B\n",
        "\n",
        "        sparse_mm = my_sparse_mm.apply # added\n",
        "\n",
        "        if K > 1:\n",
        "            x1 = sparse_mm(L,x0)              # V x Fin*B\n",
        "            x = torch.cat((x, x1.unsqueeze(0)),0)  # 2 x V x Fin*B\n",
        "        for k in range(2, K):\n",
        "            x2 = 2 * sparse_mm(L,x1) - x0\n",
        "            x = torch.cat((x, x2.unsqueeze(0)),0)  # M x Fin*B --> K x V x Fin*B\n",
        "            x0, x1 = x1, x2\n",
        "\n",
        "        x = x.view([K, V, Fin, B])           # K x V x Fin x B\n",
        "        x = x.permute(3,1,2,0).contiguous()  # B x V x Fin x K\n",
        "        x = x.view([B*V, Fin*K])             # B*V x Fin*K\n",
        "\n",
        "        # Compose linearly Fin features to get Fout features\n",
        "        x = cl(x)                            # B*V x Fout\n",
        "        x = x.view([B, V, Fout])             # B x V x Fout\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "    # Max pooling of size p. Must be a power of 2.\n",
        "    def graph_max_pool(self, x, p):\n",
        "        if p > 1:\n",
        "            x = x.permute(0,2,1).contiguous()  # x = B x F x V\n",
        "            x = nn.MaxPool1d(p)(x)             # B x F x V/p\n",
        "            x = x.permute(0,2,1).contiguous()  # x = B x V/p x F\n",
        "            return x\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "\n",
        "    def forward(self, x_in, d, L):\n",
        "        \n",
        "        x = x_in#[:,:self.num_gene]\n",
        "        x_nn = x_in#[:,self.num_gene:]\n",
        "\n",
        "        x = x.unsqueeze(2) # B x V x Fin=1\n",
        "        x = self.graph_conv_cheby(x, self.cl1, L[0], self.CL1_F, self.CL1_K)\n",
        "\n",
        "        x = F.relu(x)\n",
        "        x = self.graph_max_pool(x, self.poolsize)\n",
        "\n",
        "        # flatten()\n",
        "        x = x.view(-1, self.FC1Fin)\n",
        "\n",
        "\n",
        "        \n",
        "        ##############################################\n",
        "        ##                  GAE_re                  ##\n",
        "        ##############################################\n",
        "        x_reAdj = 0 #torch.stack([F.sigmoid(torch.mm(z_i, z_i.t())) for z_i in x_reAdj])\n",
        "        \n",
        "        ##############################################\n",
        "        ##                  GAE                     ##\n",
        "        ##############################################\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x_hidden_gae = x\n",
        "\n",
        "        x_decode_gae = self.fc2(x_hidden_gae)\n",
        "#        x_decode_gae = F.relu(x_decode_gae)\n",
        "        if self.FC2_F != 0:                \n",
        "            x_decode_gae = F.relu(x_decode_gae)\n",
        "            x_decode_gae  = nn.Dropout(d)(x_decode_gae)            \n",
        "            x_decode_gae = self.fc3(x_decode_gae)            \n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "        ##############################################\n",
        "        ##                  GCN//NN                  ##\n",
        "        ##############################################\n",
        "        \n",
        "      \n",
        "        # NN\n",
        "        x_nn = self.nn_fc1(x_nn) # B x V\n",
        "        x_nn = F.relu(x_nn)\n",
        "        x_nn = self.nn_fc2(x_nn)\n",
        "        x_nn = F.relu(x_nn)\n",
        "\n",
        "#        x_hidden_ae = x_nn\n",
        "#        x_decode_ae = self.nn_fc3(x_hidden_ae)\n",
        "#        x_decode_ae = F.relu(x_decode_ae)\n",
        "#        x_decode_ae = self.nn_fc4(x_hidden_ae)\n",
        "#        x_decode_ae = F.relu(x_decode_ae)       \n",
        "\n",
        "        # concatenate layer  \n",
        "        x = torch.cat((x_hidden_gae, x_nn),1)        \n",
        "        x = self.FC_sum2(x)\n",
        "        x = F.log_softmax(x)        \n",
        "\n",
        "        return x_decode_gae, x_hidden_gae, x, x_reAdj\n",
        "\n",
        "\n",
        "    def loss(self, y1, y_target1,y2, y_target2,l2_regularization):\n",
        "     \n",
        "        loss1 = nn.MSELoss()(y1, y_target1)\n",
        "        loss2 = nn.NLLLoss()(y2,y_target2)            \n",
        "        loss = 1 * loss1 + 1 * loss2 \n",
        "\n",
        "        l2_loss = 0.0\n",
        "        for param in self.parameters():\n",
        "            data = param* param\n",
        "            l2_loss += data.sum()\n",
        "\n",
        "\n",
        "        loss += 0.2* l2_regularization* l2_loss\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}