{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled1.ipynb","provenance":[],"authorship_tag":"ABX9TyNE/6eSDUXBDHmkc+jvVSmg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"7AwDfQ83u2z5","executionInfo":{"status":"ok","timestamp":1629567166040,"user_tz":300,"elapsed":1488,"user":{"displayName":"chris C","photoUrl":"","userId":"04517267064379487849"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from itertools import cycle\n","\n","from sklearn import svm, datasets\n","from sklearn.metrics import roc_curve, auc\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import label_binarize\n","from sklearn.multiclass import OneVsRestClassifier\n","from scipy import interp\n","from sklearn.metrics import roc_auc_score\n","\n","# Import some data to play with\n","iris = datasets.load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Binarize the output\n","y = label_binarize(y, classes=[0, 1, 2])\n","n_classes = y.shape[1]\n","\n","# Add noisy features to make the problem harder\n","random_state = np.random.RandomState(0)\n","n_samples, n_features = X.shape\n","X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n","\n","# shuffle and split training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,\n","                                                    random_state=0)\n","\n","# Learn to predict each class against the other\n","classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n","                                 random_state=random_state))\n","y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n","\n","# Compute ROC curve and ROC area for each class\n","fpr = dict()\n","tpr = dict()\n","roc_auc = dict()\n","for i in range(n_classes):\n","    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n","    roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","# Compute micro-average ROC curve and ROC area\n","fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n","roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"XXWOs-lk-2CF","executionInfo":{"status":"ok","timestamp":1629570693654,"user_tz":300,"elapsed":115,"user":{"displayName":"chris C","photoUrl":"","userId":"04517267064379487849"}}},"source":["from sklearn.linear_model import RidgeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n","\n","from yellowbrick.classifier import ROCAUC\n","from yellowbrick.datasets import load_game"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mt5CjldB-6Ff","executionInfo":{"status":"ok","timestamp":1629570820481,"user_tz":300,"elapsed":3208,"user":{"displayName":"chris C","photoUrl":"","userId":"04517267064379487849"}},"outputId":"1e50f97f-ab07-46a5-8603-e087d85479ee"},"source":["load_game()"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([(b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'x', b'o', b'b', b'b', b'b', b'b', b'x', b'o', b'x', b'o', b'x', b'o', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'win'),\n","       (b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'x', b'b', b'b', b'b', b'b', b'b', b'x', b'o', b'x', b'o', b'x', b'o', b'o', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'win'),\n","       (b'b', b'b', b'b', b'b', b'b', b'b', b'o', b'b', b'b', b'b', b'b', b'b', b'x', b'b', b'b', b'b', b'b', b'b', b'x', b'o', b'x', b'o', b'x', b'o', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'win'),\n","       ...,\n","       (b'x', b'x', b'b', b'b', b'b', b'b', b'o', b'o', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'o', b'x', b'x', b'o', b'b', b'b', b'loss'),\n","       (b'x', b'o', b'b', b'b', b'b', b'b', b'o', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'o', b'x', b'o', b'x', b'x', b'b', b'draw'),\n","       (b'x', b'o', b'o', b'o', b'x', b'b', b'o', b'b', b'b', b'b', b'b', b'b', b'x', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'b', b'x', b'b', b'b', b'b', b'b', b'b', b'draw')],\n","      dtype=[('a1', 'S1'), ('a2', 'S1'), ('a3', 'S1'), ('a4', 'S1'), ('a5', 'S1'), ('a6', 'S1'), ('b1', 'S1'), ('b2', 'S1'), ('b3', 'S1'), ('b4', 'S1'), ('b5', 'S1'), ('b6', 'S1'), ('c1', 'S1'), ('c2', 'S1'), ('c3', 'S1'), ('c4', 'S1'), ('c5', 'S1'), ('c6', 'S1'), ('d1', 'S1'), ('d2', 'S1'), ('d3', 'S1'), ('d4', 'S1'), ('d5', 'S1'), ('d6', 'S1'), ('e1', 'S1'), ('e2', 'S1'), ('e3', 'S1'), ('e4', 'S1'), ('e5', 'S1'), ('e6', 'S1'), ('f1', 'S1'), ('f2', 'S1'), ('f3', 'S1'), ('f4', 'S1'), ('f5', 'S1'), ('f6', 'S1'), ('g1', 'S1'), ('g2', 'S1'), ('g3', 'S1'), ('g4', 'S1'), ('g5', 'S1'), ('g6', 'S1'), ('outcome', 'S4')])"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244},"id":"Op_t2Wxr-wBL","executionInfo":{"status":"error","timestamp":1629570827197,"user_tz":300,"elapsed":3162,"user":{"displayName":"chris C","photoUrl":"","userId":"04517267064379487849"}},"outputId":"96cb7047-f6d4-42ab-cacf-b20c83a2811d"},"source":["from sklearn.linear_model import RidgeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n","\n","from yellowbrick.classifier import ROCAUC\n","from yellowbrick.datasets import load_game\n","\n","# Load multi-class classification dataset\n","X, y = load_game()\n","\n","# Encode the non-numeric columns\n","X = OrdinalEncoder().fit_transform(X)\n","y = LabelEncoder().fit_transform(y)\n","\n","# Create the train and test data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n","\n","# Instaniate the classification model and visualizer\n","model = RidgeClassifier()\n","visualizer = ROCAUC(model, classes=[\"win\", \"loss\", \"draw\"])\n","\n","visualizer.fit(X_train, y_train)        # Fit the training data to the visualizer\n","visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n","visualizer.show()                       # Finalize and render the figure"],"execution_count":19,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-a8d2438f2fa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load multi-class classification dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Encode the non-numeric columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"]}]},{"cell_type":"markdown","metadata":{"id":"YH1yby7OyIsP"},"source":["# Train.py\n"]},{"cell_type":"code","metadata":{"id":"lSIDJLA0yKDv","executionInfo":{"status":"ok","timestamp":1629567423936,"user_tz":300,"elapsed":701,"user":{"displayName":"chris C","photoUrl":"","userId":"04517267064379487849"}}},"source":["import time\n","import pandas as pd\n","import torch\n","#from torch.autograd import Variable\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn import metrics, preprocessing\n","from sklearn.metrics import roc_curve, auc\n","\n","import numpy as np\n","import sys\n","sys.path.insert(0, 'lib/')\n","# import utilsdata\n","\n","def calc_auc(test_labels, pred_test):\n","  test_labels = preprocessing.label_binarize(test_labels.values, classes=[0,1,2,3,4,5,6,7,8,9])\n","  pred_test = preprocessing.label_binarize(pred_test, classes=[0,1  ,2,3,4,5,6,7,8,9])\n","  n_classes = pred_test.shape[1]\n","\n","  fpr = dict()\n","  tpr = dict()\n","  roc_auc = dict()\n","  for i in range(n_classes):\n","      fpr[i], tpr[i], _ = roc_curve(test_labels[:, i], pred_test[:, i])\n","      roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","  fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(test_labels.ravel(), pred_test.ravel())\n","  roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n","\n","  return roc_auc\n","\n","\n","def calculation(pred_test, test_labels, method='GCN'):\n","    test_acc = metrics.accuracy_score(pred_test, test_labels)\n","    test_f1_macro = metrics.f1_score(pred_test, test_labels, average='macro')\n","    test_f1_micro = metrics.f1_score(pred_test, test_labels, average='micro')\n","    precision = metrics.precision_score(test_labels, pred_test, average='micro')\n","    recall = metrics.recall_score(test_labels, pred_test, average='micro')\n","    \n","    roc_auc = calc_auc(test_labels, pred_test)\n","\n","    print('method','test_acc','f1_test_macro','f1_test_micro','Testprecision','Testrecall','Testauc')\n","    print(method, test_acc, test_f1_macro, test_f1_micro, precision,recall, 'something')\n","    # print(metrics.confusion_matrix(test_labels, pred_test))\n","    print(roc_auc)\n","        \n","def weight_init(m):\n","    if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):\n","        torch.nn.init.xavier_uniform_(m.weight)\n","        if m.bias is not None: \n","            m.bias.data.fill_(0.0)\n","\n","def test_model(net, loader, L, args):\n","    t_start_test = time.time()\n","    \n","    net.eval()\n","    test_acc = 0\n","    count = 0\n","    confusionGCN = np.zeros([args.nclass, args.nclass])\n","    predictions = pd.DataFrame()\n","    y_true = []\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n","    \n","#    for batch_x, batch_y in loader:\n","#        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n","#\n","#        out_gae, out_hidden, pred, out_adj = net(batch_x, args.dropout, L)\n","#        \n","#        test_acc += utilsdata.accuracy(pred, batch_y).item()\n","#        count += 1\n","#        y_true.append(batch_y.item())\n","#        #y_pred.append(pred.max(1)[1].item())\n","#        confusionGCN[batch_y.item(), pred.max(1)[1].item()] += 1\n","#        px = pd.DataFrame(pred.detach().cpu().numpy())            \n","#        predictions = pd.concat((predictions, px),0)\n","#        \n","#    t_total_test = time.time() - t_start_test\n","#    preds_labels = np.argmax(np.asarray(predictions), 1)\n","#    test_acc = test_acc/count\n","#    predictions.insert(0, 'trueLabels', y_true)\n","\n","\n","    for batch_x, batch_y in loader:\n","        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n","\n","        out_gae, out_hidden, pred, out_adj = net(batch_x, args.dropout, L)\n","        \n","        test_acc += utilsdata.accuracy(pred, batch_y).item() * len(batch_y)\n","        count += 1\n","        y_true = batch_y.detach().cpu().numpy()\n","        y_predProbs = pred.detach().cpu().numpy()\n","        \n","    predictions = pd.DataFrame(y_predProbs)            \n","    for i in range(len(y_true)):\n","        confusionGCN[y_true[i], np.argmax(y_predProbs[i,:])] += 1\n","    \n","    t_total_test = time.time() - t_start_test\n","    preds_labels = np.argmax(np.asarray(predictions), 1)\n","    test_acc = test_acc/len(loader.dataset)\n","    predictions.insert(0, 'trueLabels', y_true)\n","\n","    \n","    return test_acc, confusionGCN, predictions, preds_labels, t_total_test\n","\n","        \n","def train_model(useModel, train_loader, val_loader, L, args):    \n","\n","    # network parameters\n","    D_g = args.num_gene\n","    CL1_F = 5\n","    CL1_K = 5\n","    FC1_F = 32\n","    FC2_F = 0\n","    NN_FC1 = 256\n","    NN_FC2 = 32\n","    out_dim = args.nclass  \n","    net_parameters = [D_g, CL1_F, CL1_K, FC1_F,FC2_F,NN_FC1, NN_FC2, out_dim]\n","\n","    # learning parameters\n","    dropout_value = 0.2\n","    l2_regularization = 5e-4\n","    batch_size = args.batchsize\n","    num_epochs = args.epochs\n","    \n","\n","    nb_iter = int(num_epochs * args.train_size) // batch_size\n","    print('num_epochs=',num_epochs,', train_size=',args.train_size,', nb_iter=',nb_iter)\n","    \n","    # Optimizer\n","    global_lr = args.lr\n","    global_step = 0\n","    decay = 0.95\n","    decay_steps = args.train_size\n","        \n","        \n","   # instantiate the object net of the class\n","    net = useModel(net_parameters)\n","    net.apply(weight_init)\n","    \n","    if torch.cuda.is_available():\n","        net.cuda()\n","        \n","    print(net)\n","            \n","    #optimizer = optim.Adam(net.parameters(),lr= args.lr, weight_decay=5e-4)\n","    optimizer = optim.SGD(net.parameters(), momentum=0.9, lr= args.lr)\n","    \n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n","    \n","    ## Train   \n","    net.train()\n","    losses_train = []\n","    acc_train = []\n","    \n","    t_total_train = time.time()\n","\n","    def adjust_learning_rate(optimizer, epoch, lr):\n","        \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n","    #    lr = args.lr * (0.1 ** (epoch // 30))\n","        lr = lr * pow( decay , float(global_step// decay_steps) )\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = lr\n","        return lr\n","    \n","    for epoch in range(num_epochs):  # loop over the dataset multiple times\n","    \n","        # update learning rate\n","        cur_lr = adjust_learning_rate(optimizer,epoch, args.lr)\n","        \n","        # reset time\n","        t_start = time.time()\n","    \n","        # extract batches\n","        epoch_loss = 0.0\n","        epoch_acc = 0.0\n","        count = 0\n","        for i, (batch_x, batch_y) in enumerate(train_loader):\n","            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n","                    \n","            optimizer.zero_grad()   \n","            out_gae, out_hidden, output, out_adj = net(batch_x, dropout_value, L)\n","\n","            loss_batch = net.loss(out_gae, batch_x, output, batch_y, l2_regularization)\n","          \n","            acc_batch = utilsdata.accuracy(output, batch_y).item()\n","            \n","            loss_batch.backward()\n","            optimizer.step()\n","            \n","            count += 1\n","            epoch_loss += loss_batch.item()\n","            epoch_acc += acc_batch\n","            global_step += args.batchsize \n","            \n","            # print\n","            if count % 1000 == 0: # print every x mini-batches\n","                print('epoch= %d, i= %4d, loss(batch)= %.4f, accuray(batch)= %.2f' % (epoch + 1, count, loss_batch.item(), acc_batch))\n","    \n","    \n","        epoch_loss /= count\n","        epoch_acc /= count\n","        losses_train.append(epoch_loss) # Calculating the loss\n","        acc_train.append(epoch_acc) # Calculating the acc\n","        # print\n","        t_stop = time.time() - t_start\n","        \n","        if epoch % 10 == 0 and epoch != 0:\n","            with torch.no_grad():\n","                val_acc = 0  \n","                count = 0\n","                for b_x, b_y in val_loader:\n","                    b_x, b_y = b_x.to(device), b_y.to(device)          \n","                    _, _, val_pred, _ = net(b_x, args.dropout, L)                    \n","                    val_acc += utilsdata.accuracy(val_pred, b_y).item() * len(b_y)\n","                    count += 1\n","                    \n","                val_acc = val_acc/len(val_loader.dataset)\n","                \n","            print('epoch= %d, loss(train)= %.3f, accuracy(train)= %.3f, time= %.3f, lr= %.5f' %\n","                  (epoch + 1, epoch_loss, epoch_acc, t_stop, cur_lr))\n","            print('----accuracy(val)= ', val_acc)\n","            print('training_time:',t_stop)\n","        else:\n","            print('epoch= %d, loss(train)= %.3f, accuracy(train)= %.3f, time= %.3f, lr= %.5f' %\n","                  (epoch + 1, epoch_loss, epoch_acc, t_stop, cur_lr))\n","            print('training_time:',t_stop)\n","        \n","    \n","    t_total_train = time.time() - t_total_train  \n","    \n","    return net, t_total_train\n"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cbc6AQQxyaS8"},"source":["# siggcn.py"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":407},"id":"0nXI4lyHyiNx","executionInfo":{"status":"error","timestamp":1629567466276,"user_tz":300,"elapsed":418,"user":{"displayName":"chris C","photoUrl":"","userId":"04517267064379487849"}},"outputId":"dc6fc066-de1f-45c1-cb72-860cbe5cfc57"},"source":["import sys, os\n","import torch\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import torch.utils.data as Data\n","import torch.optim as optim\n","import argparse\n","import time\n","import numpy as np\n","\n","import scipy.sparse as sp\n","from scipy.sparse import csr_matrix\n","\n","import pandas as pd\n","import sys\n","sys.path.insert(0, 'lib/')\n","\n","\n","if torch.cuda.is_available():\n","    print('cuda available')\n","    dtypeFloat = torch.cuda.FloatTensor\n","    dtypeLong = torch.cuda.LongTensor\n","    torch.cuda.manual_seed(1)\n","else:\n","    print('cuda not available')\n","    dtypeFloat = torch.FloatTensor\n","    dtypeLong = torch.LongTensor\n","    torch.manual_seed(1)\n","\n","from coarsening import coarsen, laplacian\n","from coarsening import lmax_L\n","from coarsening import perm_data\n","from coarsening import rescale_L\n","from layermodel import *\n","import utilsdata\n","from utilsdata import *\n","from train import *\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","#\n","#\n","# Directories.\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--dirData', type=str, default='/Users/tianyu/Desktop/scRNAseq_Benchmark_datasets/Intra-dataset/', help=\"directory of cell x gene matrix\")\n","parser.add_argument('--dataset', type=str, default='Zhengsorted', help=\"dataset\")\n","parser.add_argument('--dirAdj', type = str, default = '/Users/tianyu/Desktop/scRNAseq_Benchmark_datasets/Intra-dataset/Zhengsorted/', help = 'directory of adj matrix')\n","parser.add_argument('--dirLabel', type = str, default = '/Users/tianyu/Desktop/scRNAseq_Benchmark_datasets/Intra-dataset/Zhengsorted/', help = 'directory of adj matrix')\n","parser.add_argument('--outputDir', type = str, default = 'data/output', help = 'directory to save results')\n","parser.add_argument('--saveResults', type=int, default = 0, help='whether or not save the results')\n","\n","parser.add_argument('--normalized_laplacian', type=bool, default = True, help='Graph Laplacian: normalized.')\n","parser.add_argument('--lr', type=float, default = 0.01, help='learning rate.')\n","parser.add_argument('--num_gene', type=int, default = 1000, help='# of genes')\n","parser.add_argument('--epochs', type=int, default = 1, help='# of epoch')\n","parser.add_argument('--batchsize', type=int, default = 64, help='# of genes')\n","parser.add_argument('--dropout', type=float, default = 0.2, help='dropout value')\n","parser.add_argument('--id1', type=str, default = '', help='test in pancreas')\n","parser.add_argument('--id2', type=str, default = '', help='test in pancreas')\n","\n","parser.add_argument('--net', type=str, default='String', help=\"netWork\")\n","parser.add_argument('--dist', type=str, default='', help=\"dist type\")\n","parser.add_argument('--sampling_rate', type=float, default = 1, help='# sampling rate of cells')\n","\n","args = parser.parse_args()\n","\n","t_start = time.process_time()\n","\n","\n","# Load data\n","\n","\n","print('load data...')    \n","adjall, alldata, labels, shuffle_index = utilsdata.load_largesc(path = args.dirData, dirAdj=args.dirAdj, dataset=args.dataset, net='String')\n","\n","# generate a fixed shuffle index\n","if shuffle_index is not None:\n","    shuffle_index = shuffle_index.astype(np.int32)\n","else:\n","    shuffle_index = np.random.permutation(alldata.shape[0])\n","    np.savetxt(args.dirData +'/' + args.dataset +'/shuffle_index_'+args.dataset+'.txt')\n","    \n","train_all_data, adj = utilsdata.down_genes(alldata, adjall, args.num_gene)\n","L = [laplacian(adj, normalized=True)]\n","\n","\n","#####################################################\n","\n","##Split the dataset into train, val, test dataset. Use a fixed shuffle index to fix the sample order for comparison.\n","train_data, val_data, test_data, train_labels, val_labels, test_labels = utilsdata.spilt_dataset(train_all_data, labels, shuffle_index)\n","args.nclass = len(np.unique(labels))\n","args.train_size = train_data.shape[0] \n","\n","## Use the train_data, val_data, test_data to generate the train, val, test loader\n","train_loader, val_loader, test_loader = utilsdata.generate_loader(train_data,val_data, test_data, \n","                                                        train_labels, val_labels, test_labels, \n","                                                        args.batchsize)\n","\n","\n","\n","\n","##Delete existing network if exists\n","try:\n","    del net\n","    print('Delete existing network\\n')\n","except NameError:\n","    print('No existing network to delete\\n')\n","\n","# Train model\n","net, t_total_train = train_model(Graph_GCN, train_loader,val_loader, L, args)\n","\n","## Val\n","val_acc,confusionGCN, predictions, preds_labels, t_total_test = test_model(net, val_loader, L, args)\n","print('  accuracy(val) = %.3f %%, time= %.3f' % (val_acc, t_total_test))\n","\n","# Test\n","test_acc,confusionGCN, predictions, preds_labels, t_total_test = test_model(net, test_loader, L, args)\n","    \n","print('  accuracy(test) = %.3f %%, time= %.3f' % (test_acc, t_total_test))\n","# fpr, tpr, auc = calculation(preds_labels, predictions.iloc[:,0])\n","calculation(preds_labels, predictions.iloc[:,0])\n","\n","if args.saveResults:\n","    testPreds4save = pd.DataFrame(preds_labels,columns=['predLabels'])\n","    testPreds4save.insert(0, 'trueLabels', list(predictions.iloc[:,0]))\n","    confusionGCN = pd.DataFrame(confusionGCN)\n","    \n","    testPreds4save.to_csv(args.outputDir+'/gcn_test_preds_'+ args.dataset+ str(args.num_gene)+'.csv')\n","    predictions.to_csv(args.outputDir+'/gcn_testProbs_preds_'+ args.dataset+ str(args.num_gene) +'.csv')\n","    confusionGCN.to_csv(args.outputDir+'/gcn_confuMat_'+ args.dataset+ str(args.num_gene)+'.csv')    \n","    np.savetxt(args.outputDir+'/newgcn_train_time_'+args.dataset + str(args.num_gene) +'.txt', [t_total_train])   \n","    np.savetxt(args.outputDir+'/newgcn_test_time_'+args.dataset + str(args.num_gene) +'.txt', [t_total_test])\n","\n","# print(fpr.tolist())\n","# print(tpr.tolist())\n","# print(auc)\n","\n","# plt.title('Receiver Operating Characteristic')\n","# plt.plot(fpr.tolist(), tpr.tolist(), 'b', label = 'AUC = %0.2f' % auc)\n","# plt.legend(loc = 'lower right')\n","# plt.plot([0, 1], [0, 1],'r--')\n","# plt.xlim([0, 1])\n","# plt.ylim([0, 1])\n","# plt.ylabel('True Positive Rate')\n","# plt.xlabel('False Positive Rate')\n","# plt.show()\n","# plt.savefig('foo.png')\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["cuda not available\n"],"name":"stdout"},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-c7e7ed9b6ac5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcoarsening\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoarsen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlaplacian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcoarsening\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlmax_L\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcoarsening\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mperm_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'coarsening'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]}]}