{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"sigGCN_running.ipynb","provenance":[],"collapsed_sections":["tk0sw_2ORdZR"],"machine_shape":"hm","authorship_tag":"ABX9TyNsIdwTLGIHRCP1k6omN+UA"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OmgdFkO7DsSk","executionInfo":{"elapsed":25886,"status":"ok","timestamp":1629744795347,"user":{"displayName":"chris C","photoUrl":"","userId":"04517267064379487849"},"user_tz":300},"outputId":"41475863-bf9d-4381-8fd8-7d1940052729"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HR9Y4H67D11t","executionInfo":{"elapsed":347,"status":"ok","timestamp":1629744821981,"user":{"displayName":"chris C","photoUrl":"","userId":"04517267064379487849"},"user_tz":300},"outputId":"7fcab492-9de6-4344-88d2-ccec4fc4805f"},"source":["#changing the working directory\n","%cd /content/gdrive/MyDrive/Github/sigGCN-main/"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["/content/gdrive/MyDrive/Github/sigGCN-main\n"]}]},{"cell_type":"markdown","metadata":{"id":"8uw1Te4mdqrG"},"source":["# BaronHuman"]},{"cell_type":"code","metadata":{"id":"gW8Twsg6dt87"},"source":["!python siggcn.py --dirData=\"data/\" --dataset=\"BaronHuman\" --dirAdj=\"data/BaronHuman/\" --dirLabel=\"data/BaronHuman/\" --outputDir=\"data/output\" --saveResults=0 --epochs=250 --batchsize=30"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bQZWNI3rdmKJ"},"source":["# Zhengsorted"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"vUClOFx1D6D9"},"source":["!python siggcn.py --dirData=\"data/\" --dataset=\"Zhengsorted\" --dirAdj=\"data/Zhengsorted/\" --dirLabel=\"data/Zhengsorted/\" --outputDir=\"data/output\" --saveResults=0 --epochs=1\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-CKDLVqcD14r"},"source":["%pycat train.py\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7z4I1Rq4CvrA","executionInfo":{"elapsed":765,"status":"ok","timestamp":1629615691615,"user":{"displayName":"chris C","photoUrl":"","userId":"04517267064379487849"},"user_tz":300},"outputId":"ad62801f-65ee-4246-eb11-9d47e9c21edc"},"source":["%%writefile train.py\n","#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Tue Mar 23 18:30:20 2021\n","\n","@author: tianyu\n","\"\"\"\n","import time\n","import pandas as pd\n","import torch\n","#from torch.autograd import Variable\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn import metrics, preprocessing\n","from sklearn.metrics import roc_curve, auc\n","\n","import numpy as np\n","import sys\n","sys.path.insert(0, 'lib/')\n","import utilsdata\n","\n","def calc_auc(test_labels, preds_probs, dataset):\n","  if (dataset == 'Zhengsorted'):\n","    classes = [0,1,2,3,4,5,6,7,8,9]\n","  elif (dataset == 'BaronHuman'):\n","    classes = [0,1,2,3,4,5,6,7,8,9,10,11,12,13]\n","  \n","  test_labels = preprocessing.label_binarize(test_labels.values, classes=classes)\n","\n","  n_classes = test_labels.shape[1]\n","  print('n_classes: ', n_classes)\n","\n","  fpr = dict()\n","  tpr = dict()\n","  roc_auc = dict()\n","  for i in range(n_classes):\n","      fpr[i], tpr[i], _ = roc_curve(test_labels[:, i], preds_probs[:, i])\n","      roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","  fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(test_labels.ravel(), preds_probs.ravel())\n","  roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n","\n","  return fpr, tpr, roc_auc\n","\n","\n","def calculation(pred_test, test_labels, preds_probs, dataset, method='GCN'):\n","    test_acc = metrics.accuracy_score(pred_test, test_labels)\n","    test_f1_macro = metrics.f1_score(pred_test, test_labels, average='macro')\n","    test_f1_micro = metrics.f1_score(pred_test, test_labels, average='micro')\n","    precision = metrics.precision_score(test_labels, pred_test, average='micro')\n","    recall = metrics.recall_score(test_labels, pred_test, average='micro')\n","    \n","    # print('test_labels', test_labels.shape)\n","    # print('preds_probs', preds_probs.shape)\n","\n","    fpr, tpr, roc_auc = calc_auc(test_labels, preds_probs, dataset)\n","\n","    # print('method','test_acc','f1_test_macro','f1_test_micro','Testprecision','Testrecall','Testauc')\n","    # print(method, test_acc, test_f1_macro, test_f1_micro, precision,recall, 'something')\n","    # print(metrics.confusion_matrix(test_labels, pred_test))\n","    # print(fpr)\n","    # print()\n","    # print(tpr)\n","    # print()\n","    # print(roc_auc)\n","    return fpr, tpr, roc_auc\n","        \n","def weight_init(m):\n","    if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):\n","        torch.nn.init.xavier_uniform_(m.weight)\n","        if m.bias is not None: \n","            m.bias.data.fill_(0.0)\n","\n","def test_model(net, loader, L, args):\n","    t_start_test = time.time()\n","    \n","    net.eval()\n","    test_acc = 0\n","    count = 0\n","    confusionGCN = np.zeros([args.nclass, args.nclass])\n","    predictions = pd.DataFrame()\n","    y_true = []\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n","    \n","#    for batch_x, batch_y in loader:\n","#        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n","#\n","#        out_gae, out_hidden, pred, out_adj = net(batch_x, args.dropout, L)\n","#        \n","#        test_acc += utilsdata.accuracy(pred, batch_y).item()\n","#        count += 1\n","#        y_true.append(batch_y.item())\n","#        #y_pred.append(pred.max(1)[1].item())\n","#        confusionGCN[batch_y.item(), pred.max(1)[1].item()] += 1\n","#        px = pd.DataFrame(pred.detach().cpu().numpy())            \n","#        predictions = pd.concat((predictions, px),0)\n","#        \n","#    t_total_test = time.time() - t_start_test\n","#    preds_labels = np.argmax(np.asarray(predictions), 1)\n","#    test_acc = test_acc/count\n","#    predictions.insert(0, 'trueLabels', y_true)\n","\n","\n","    for batch_x, batch_y in loader:\n","        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n","\n","        out_gae, out_hidden, pred, out_adj = net(batch_x, args.dropout, L)\n","        \n","        test_acc += utilsdata.accuracy(pred, batch_y).item() * len(batch_y)\n","        count += 1\n","        y_true = batch_y.detach().cpu().numpy()\n","        y_predProbs = pred.detach().cpu().numpy()\n","        \n","    predictions = pd.DataFrame(y_predProbs)            \n","    for i in range(len(y_true)):\n","        confusionGCN[y_true[i], np.argmax(y_predProbs[i,:])] += 1\n","    \n","    t_total_test = time.time() - t_start_test\n","    preds_labels = np.argmax(np.asarray(predictions), 1)\n","    preds_probs = np.asarray(predictions)\n","    test_acc = test_acc/len(loader.dataset)\n","    predictions.insert(0, 'trueLabels', y_true)\n","    n_classes = preds_probs.shape[1]\n","    \n","    return test_acc, confusionGCN, predictions, preds_labels, t_total_test, preds_probs, n_classes\n","\n","        \n","def train_model(useModel, train_loader, val_loader, L, args):    \n","\n","    # network parameters\n","    D_g = args.num_gene\n","    CL1_F = 5\n","    CL1_K = 5\n","    FC1_F = 32\n","    FC2_F = 0\n","    NN_FC1 = 256\n","    NN_FC2 = 32\n","    out_dim = args.nclass  \n","    net_parameters = [D_g, CL1_F, CL1_K, FC1_F,FC2_F,NN_FC1, NN_FC2, out_dim]\n","\n","    # learning parameters\n","    dropout_value = 0.2\n","    l2_regularization = 5e-4\n","    batch_size = args.batchsize\n","    num_epochs = args.epochs\n","    \n","\n","    nb_iter = int(num_epochs * args.train_size) // batch_size\n","    print('num_epochs=',num_epochs,', train_size=',args.train_size,', nb_iter=',nb_iter)\n","    \n","    # Optimizer\n","    global_lr = args.lr\n","    global_step = 0\n","    decay = 0.95\n","    decay_steps = args.train_size\n","        \n","        \n","   # instantiate the object net of the class\n","    net = useModel(net_parameters)\n","    net.apply(weight_init)\n","    \n","    if torch.cuda.is_available():\n","        net.cuda()\n","        \n","    print(net)\n","            \n","    #optimizer = optim.Adam(net.parameters(),lr= args.lr, weight_decay=5e-4)\n","    optimizer = optim.SGD(net.parameters(), momentum=0.9, lr= args.lr)\n","    \n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n","    \n","    ## Train   \n","    net.train()\n","    losses_train = []\n","    acc_train = []\n","    \n","    t_total_train = time.time()\n","\n","    def adjust_learning_rate(optimizer, epoch, lr):\n","        \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n","    #    lr = args.lr * (0.1 ** (epoch // 30))\n","        lr = lr * pow( decay , float(global_step// decay_steps) )\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = lr\n","        return lr\n","    \n","    for epoch in range(num_epochs):  # loop over the dataset multiple times\n","    \n","        # update learning rate\n","        cur_lr = adjust_learning_rate(optimizer,epoch, args.lr)\n","        \n","        # reset time\n","        t_start = time.time()\n","    \n","        # extract batches\n","        epoch_loss = 0.0\n","        epoch_acc = 0.0\n","        count = 0\n","        for i, (batch_x, batch_y) in enumerate(train_loader):\n","            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n","                    \n","            optimizer.zero_grad()   \n","            out_gae, out_hidden, output, out_adj = net(batch_x, dropout_value, L)\n","\n","            loss_batch = net.loss(out_gae, batch_x, output, batch_y, l2_regularization)\n","          \n","            acc_batch = utilsdata.accuracy(output, batch_y).item()\n","            \n","            loss_batch.backward()\n","            optimizer.step()\n","            \n","            count += 1\n","            epoch_loss += loss_batch.item()\n","            epoch_acc += acc_batch\n","            global_step += args.batchsize \n","            \n","            # print\n","            if count % 1000 == 0: # print every x mini-batches\n","                print('epoch= %d, i= %4d, loss(batch)= %.4f, accuray(batch)= %.2f' % (epoch + 1, count, loss_batch.item(), acc_batch))\n","    \n","    \n","        epoch_loss /= count\n","        epoch_acc /= count\n","        losses_train.append(epoch_loss) # Calculating the loss\n","        acc_train.append(epoch_acc) # Calculating the acc\n","        # print\n","        t_stop = time.time() - t_start\n","        \n","        if epoch % 10 == 0 and epoch != 0:\n","            with torch.no_grad():\n","                val_acc = 0  \n","                count = 0\n","                for b_x, b_y in val_loader:\n","                    b_x, b_y = b_x.to(device), b_y.to(device)          \n","                    _, _, val_pred, _ = net(b_x, args.dropout, L)                    \n","                    val_acc += utilsdata.accuracy(val_pred, b_y).item() * len(b_y)\n","                    count += 1\n","                    \n","                val_acc = val_acc/len(val_loader.dataset)\n","                \n","            print('epoch= %d, loss(train)= %.3f, accuracy(train)= %.3f, time= %.3f, lr= %.5f' %\n","                  (epoch + 1, epoch_loss, epoch_acc, t_stop, cur_lr))\n","            print('----accuracy(val)= ', val_acc)\n","            print('training_time:',t_stop)\n","        else:\n","            print('epoch= %d, loss(train)= %.3f, accuracy(train)= %.3f, time= %.3f, lr= %.5f' %\n","                  (epoch + 1, epoch_loss, epoch_acc, t_stop, cur_lr))\n","            print('training_time:',t_stop)\n","        \n","    \n","    t_total_train = time.time() - t_total_train  \n","    \n","    return net, t_total_train\n"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting train.py\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ctZe1dRuD17l","executionInfo":{"elapsed":756,"status":"ok","timestamp":1629616354232,"user":{"displayName":"chris C","photoUrl":"","userId":"04517267064379487849"},"user_tz":300},"outputId":"86a694eb-455e-40ca-9677-c0b6cdc0a7a1"},"source":["%%writefile siggcn.py\n","#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Sun Mar 22 14:00:37 2020\n","\n","@author: tianyu\n","\"\"\"\n","   \n","import sys, os\n","import torch\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import torch.utils.data as Data\n","import torch.optim as optim\n","import argparse\n","import time\n","import numpy as np\n","\n","import scipy.sparse as sp\n","from scipy.sparse import csr_matrix\n","\n","\n","from sklearn.preprocessing import label_binarize\n","\n","import pandas as pd\n","import sys\n","sys.path.insert(0, 'lib/')\n","\n","\n","if torch.cuda.is_available():\n","    print('cuda available')\n","    dtypeFloat = torch.cuda.FloatTensor\n","    dtypeLong = torch.cuda.LongTensor\n","    torch.cuda.manual_seed(1)\n","else:\n","    print('cuda not available')\n","    dtypeFloat = torch.FloatTensor\n","    dtypeLong = torch.LongTensor\n","    torch.manual_seed(1)\n","\n","from coarsening import coarsen, laplacian\n","from coarsening import lmax_L\n","from coarsening import perm_data\n","from coarsening import rescale_L\n","from layermodel import *\n","import utilsdata\n","from utilsdata import *\n","from train import *\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","#\n","#\n","# Directories.\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--dirData', type=str, default='/Users/tianyu/Desktop/scRNAseq_Benchmark_datasets/Intra-dataset/', help=\"directory of cell x gene matrix\")\n","parser.add_argument('--dataset', type=str, default='Zhengsorted', help=\"dataset\")\n","parser.add_argument('--dirAdj', type = str, default = '/Users/tianyu/Desktop/scRNAseq_Benchmark_datasets/Intra-dataset/Zhengsorted/', help = 'directory of adj matrix')\n","parser.add_argument('--dirLabel', type = str, default = '/Users/tianyu/Desktop/scRNAseq_Benchmark_datasets/Intra-dataset/Zhengsorted/', help = 'directory of adj matrix')\n","parser.add_argument('--outputDir', type = str, default = 'data/output', help = 'directory to save results')\n","parser.add_argument('--saveResults', type=int, default = 0, help='whether or not save the results')\n","\n","parser.add_argument('--normalized_laplacian', type=bool, default = True, help='Graph Laplacian: normalized.')\n","parser.add_argument('--lr', type=float, default = 0.01, help='learning rate.')\n","parser.add_argument('--num_gene', type=int, default = 1000, help='# of genes')\n","parser.add_argument('--epochs', type=int, default = 1, help='# of epoch')\n","parser.add_argument('--batchsize', type=int, default = 64, help='# of genes')\n","parser.add_argument('--dropout', type=float, default = 0.2, help='dropout value')\n","parser.add_argument('--id1', type=str, default = '', help='test in pancreas')\n","parser.add_argument('--id2', type=str, default = '', help='test in pancreas')\n","\n","parser.add_argument('--net', type=str, default='String', help=\"netWork\")\n","parser.add_argument('--dist', type=str, default='', help=\"dist type\")\n","parser.add_argument('--sampling_rate', type=float, default = 1, help='# sampling rate of cells')\n","\n","args = parser.parse_args()\n","\n","t_start = time.process_time()\n","\n","\n","# Load data\n","\n","\n","print('load data...')    \n","adjall, alldata, labels, shuffle_index = utilsdata.load_largesc(path = args.dirData, dirAdj=args.dirAdj, dataset=args.dataset, net='String')\n","\n","# generate a fixed shuffle index\n","if shuffle_index is not None:\n","    shuffle_index = shuffle_index.astype(np.int32)\n","else:\n","    shuffle_index = np.random.permutation(alldata.shape[0])\n","    np.savetxt(args.dirData +'/' + args.dataset +'/shuffle_index_'+args.dataset+'.txt')\n","    \n","train_all_data, adj = utilsdata.down_genes(alldata, adjall, args.num_gene)\n","L = [laplacian(adj, normalized=True)]\n","\n","\n","#####################################################\n","\n","##Split the dataset into train, val, test dataset. Use a fixed shuffle index to fix the sample order for comparison.\n","train_data, val_data, test_data, train_labels, val_labels, test_labels = utilsdata.spilt_dataset(train_all_data, labels, shuffle_index)\n","args.nclass = len(np.unique(labels))\n","args.train_size = train_data.shape[0] \n","\n","# train_labels = label_binarize(train_labels, classes=[0,1,2,3,4,5,6,7,8,9])\n","# val_labels = label_binarize(val_labels, classes=[0,1,2,3,4,5,6,7,8,9])\n","# test_labels = label_binarize(test_labels, classes=[0,1,2,3,4,5,6,7,8,9])\n","\n","## Use the train_data, val_data, test_data to generate the train, val, test loader\n","train_loader, val_loader, test_loader = utilsdata.generate_loader(train_data,val_data, test_data, \n","                                                        train_labels, val_labels, test_labels, \n","                                                        args.batchsize)\n","\n","\n","\n","\n","##Delete existing network if exists\n","try:\n","    del net\n","    print('Delete existing network\\n')\n","except NameError:\n","    print('No existing network to delete\\n')\n","\n","# Train model\n","net, t_total_train = train_model(Graph_GCN, train_loader,val_loader, L, args)\n","\n","## Val\n","val_acc,confusionGCN, predictions, preds_labels, t_total_test, preds_probs, n_classes = test_model(net, val_loader, L, args)\n","print('  accuracy(val) = %.3f %%, time= %.3f' % (val_acc, t_total_test))\n","\n","# Test\n","test_acc,confusionGCN, predictions, preds_labels, t_total_test, preds_probs, n_classes = test_model(net, test_loader, L, args)\n","    \n","print('  accuracy(test) = %.3f %%, time= %.3f' % (test_acc, t_total_test))\n","fpr, tpr, roc_auc = calculation(preds_labels, predictions.iloc[:,0], preds_probs, args.dataset)\n","# calculation(preds_labels, predictions.iloc[:,0], preds_probs)\n","\n","if args.saveResults:\n","    testPreds4save = pd.DataFrame(preds_labels,columns=['predLabels'])\n","    testPreds4save.insert(0, 'trueLabels', list(predictions.iloc[:,0]))\n","    confusionGCN = pd.DataFrame(confusionGCN)\n","    \n","    testPreds4save.to_csv(args.outputDir+'/gcn_test_preds_'+ args.dataset+ str(args.num_gene)+'.csv')\n","    predictions.to_csv(args.outputDir+'/gcn_testProbs_preds_'+ args.dataset+ str(args.num_gene) +'.csv')\n","    confusionGCN.to_csv(args.outputDir+'/gcn_confuMat_'+ args.dataset+ str(args.num_gene)+'.csv')    \n","    np.savetxt(args.outputDir+'/newgcn_train_time_'+args.dataset + str(args.num_gene) +'.txt', [t_total_train])   \n","    np.savetxt(args.outputDir+'/newgcn_test_time_'+args.dataset + str(args.num_gene) +'.txt', [t_total_test])\n","\n","import matplotlib.pyplot as plt\n","import itertools\n","from matplotlib.font_manager import FontProperties\n","\n","fontP = FontProperties()\n","fontP.set_size('x-small')\n","\n","plt.figure()\n","plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n","         label='micro-average ROC curve (area = {0:0.3f})'\n","               ''.format(roc_auc[\"micro\"]),\n","         color='r', linestyle=':', linewidth=2)\n","\n","colors = itertools.cycle(['aqua', 'fuchsia', 'darkorange', 'indigo', 'yellow', 'darkgreen',\n","                          'teal', 'lime', 'brown', 'cadetblue', 'darkred', 'slategrey', 'purple', 'olive', \n","                          'chocolate', 'crimson', 'lawngreen'])\n","\n","if (args.dataset == 'Zhengsorted'):\n","  celltypes = ['CD14+ Monocyte', 'CD19+ B', 'CD34+ (Pos_label)', 'CD4+ T Helper2', 'CD4+/CD25 T Reg',\n","          'CD4+/CD45RA+/CD25- Naive T', 'CD4+/CD45RO+ Memory', 'CD56+ NK', 'CD8+ Cytotoxic T',\n","          'CD8+/CD45RA+ Naive Cytotoxic']\n","elif (args.dataset == 'BaronHuman'):\n","  celltypes = ['acinar', 'activated_stellate', 'alpha', 'beta', 'delta', 'ductal', 'endothelial', 'epsilon',\n","               'gamma', 'macrophage', 'mast', 'quiescent_stellate', 'schwann', 't_cell']\n","\n","for i, color, cell in zip(range(n_classes), colors, celltypes):\n","    lw = 2\n","    plt.plot(fpr[i], tpr[i], color=color,\n","            lw=lw, label='{0} (area {1:0.3f})'\n","            .format(cell, roc_auc[i]))\n","# plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver operating characteristic')\n","plt.legend(loc=\"lower right\", prop=fontP)\n","plt.savefig('roc_auc_{}.png'.format(args.dataset))\n"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting siggcn.py\n"]}]},{"cell_type":"markdown","metadata":{"id":"tk0sw_2ORdZR"},"source":["# New Section"]},{"cell_type":"code","metadata":{"id":"Efe6dQb06HnU"},"source":["%%writefile lib/layermodel.py\n","\n","#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Mon Jan 13 21:15:43 2020\n","\n","@author: tianyu\n","\"\"\"\n","import torch\n","#from torch.autograd import Variable\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import numpy as np\n","import sys\n","sys.path.insert(0, 'lib/')\n","\n","\n","if torch.cuda.is_available():\n","    print('cuda available')\n","    dtypeFloat = torch.cuda.FloatTensor\n","    dtypeLong = torch.cuda.LongTensor\n","    torch.cuda.manual_seed(1)\n","else:\n","    print('cuda not available')\n","    dtypeFloat = torch.FloatTensor\n","    dtypeLong = torch.LongTensor\n","    torch.manual_seed(1)\n","\n","from coarsening import lmax_L\n","from coarsening import rescale_L\n","from utilsdata import sparse_mx_to_torch_sparse_tensor\n","\n","class my_sparse_mm(torch.autograd.Function):\n","    \"\"\"\n","    Implementation of a new autograd function for sparse variables,\n","    called \"my_sparse_mm\", by subclassing torch.autograd.Function\n","    and implementing the forward and backward passes.\n","    \"\"\"\n","\n","    @staticmethod\n","    def forward(ctx, W, x):  # W is SPARSE\n","        ctx.save_for_backward(W, x)\n","        y = torch.mm(W, x)\n","        return y\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        W, x = ctx.saved_tensors\n","        grad_input = grad_output.clone()\n","        grad_input_dL_dW = torch.mm(grad_input, x.t())\n","        grad_input_dL_dx = torch.mm(W.t(), grad_input )\n","        return grad_input_dL_dW, grad_input_dL_dx\n","\n","\n","\n","\n","#########################################################################################################\n","class Graph_GCN(nn.Module):\n","\n","    def __init__(self, net_parameters):\n","\n","        print('Graph ConvNet: GCN')\n","\n","        super(Graph_GCN, self).__init__()\n","\n","        # parameters\n","        D_g, CL1_F, CL1_K,  FC1_F, FC2_F,NN_FC1, NN_FC2, out_dim = net_parameters\n","        CNN1_F, CNN1_K = 32, 5\n","        CL2_F, CL2_K = 10, 10\n","        D_nn = D_g\n","        self.in_dim = D_g\n","        self.out_dim = out_dim\n","        self.FC2_F = FC2_F\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.num_gene = D_nn\n","        self.initScale = initScale = 6\n","        self.poolsize = 8\n","        FC1Fin = CL1_F*(D_g//self.poolsize)\n","        self.FC1Fin = FC1Fin\n","        self.CL1_K = CL1_K; self.CL1_F = CL1_F; \n","        \n","        # Feature_H, Feature_W = (Input_Height - filter_H + 2P)/S + 1, (Input_Width - filter_W + 2P)/S + 1\n","        height = int(np.ceil(np.sqrt(int(D_nn))))\n","        FC2Fin = int(CNN1_F * (height//2) ** 2)\n","        self.FC2Fin = FC2Fin;\n","        \n","        # graph CL1\n","        self.cl1 = nn.Linear(CL1_K, CL1_F)\n","#        # graph CL2\n","#        self.cl2 = nn.Linear(CL2_K*CL1_F, CL2_F)\n","#        #FC gcnpure\n","#        self.fc_gcnpure = nn.Linear(FC1Fin, self.out_dim)\n","        # FC 1\n","        self.fc1 = nn.Linear(FC1Fin, FC1_F)\n","        # FC 2\n","        if self.FC2_F == 0:\n","            FC2_F = self.num_gene\n","            print('---------',FC2_F)\n","        self.fc2 = nn.Linear(FC1_F, FC2_F)\n","        # FC 3\n","        self.fc3 = nn.Linear(FC2_F, D_g)\n","        # CNN_FC1\n","        self.cnn_fc1 = nn.Linear(FC2Fin, FC1_F)\n","        #FC_concat with CNN\n","        Fin = FC1Fin + FC2Fin; Fout = self.out_dim;\n","        self.FC_concat = nn.Linear(Fin, self.out_dim)             \n","        #FC_sum2 with NN\n","        Fin = FC1_F + NN_FC2; Fout = self.out_dim;\n","        self.FC_sum2 = nn.Linear(Fin, Fout)                  \n","        #FC_sum1 with CNN\n","        Fin = FC1_F + FC1_F; Fout = self.out_dim;\n","        self.FC_sum1 = nn.Linear(Fin, Fout)             \n","        # NN_FC1\n","        self.nn_fc1 = nn.Linear(self.in_dim, NN_FC1)\n","        # NN_FC2\n","        self.nn_fc2 = nn.Linear(NN_FC1, NN_FC2)\n","        # NN_FC3_decode\n","        self.nn_fc3 = nn.Linear(NN_FC2, NN_FC1)\n","        # NN_FC4_decode\n","        Fin = NN_FC2; Fout = self.in_dim;\n","        self.nn_fc4 = nn.Linear(Fin, Fout)        \n","\n","        \n","        # nb of parameters\n","        nb_param = CL1_K* CL1_F + CL1_F          # CL1\n","#        nb_param += CL2_K* CL1_F* CL2_F + CL2_F  # CL2\n","        nb_param += FC1Fin* FC1_F + FC1_F        # FC1\n","#        nb_param += FC1_F* FC2_F + FC2_F         # FC2\n","        print('nb of parameters=',nb_param,'\\n')\n","\n","\n","    def init_weights(self, W, Fin, Fout):\n","\n","        scale = np.sqrt( self.initScale / (Fin+Fout) )\n","        W.uniform_(-scale, scale)\n","        \n","        return W\n","\n","\n","    def graph_conv_cheby(self, x, cl,L, Fout, K):\n","\n","        # parameters\n","        # B = batch size\n","        # V = nb vertices\n","        # Fin = nb input features\n","        # Fout = nb output features\n","        # K = Chebyshev order & support size\n","        B, V, Fin = x.size(); B, V, Fin = int(B), int(V), int(Fin)\n","\n","        # rescale Laplacian\n","        lmax = lmax_L(L)\n","        L = rescale_L(L, lmax)\n","\n","        # convert scipy sparse matric L to pytorch\n","        L = sparse_mx_to_torch_sparse_tensor(L)\n","        if torch.cuda.is_available():\n","            L = L.cuda()\n","\n","        # transform to Chebyshev basis\n","        x0 = x.permute(1,2,0).contiguous()  # V x Fin x B\n","        x0 = x0.view([V, Fin*B])            # V x Fin*B\n","        x = x0.unsqueeze(0)                 # 1 x V x Fin*B\n","\n","        def concat(x, x_):\n","            x_ = x_.unsqueeze(0)            # 1 x V x Fin*B\n","            return torch.cat((x, x_), 0)    # K x V x Fin*B\n","\n","        sparse_mm = my_sparse_mm.apply # added\n","\n","        if K > 1:\n","            x1 = sparse_mm(L,x0)              # V x Fin*B\n","            x = torch.cat((x, x1.unsqueeze(0)),0)  # 2 x V x Fin*B\n","        for k in range(2, K):\n","            x2 = 2 * sparse_mm(L,x1) - x0\n","            x = torch.cat((x, x2.unsqueeze(0)),0)  # M x Fin*B --> K x V x Fin*B\n","            x0, x1 = x1, x2\n","\n","        x = x.view([K, V, Fin, B])           # K x V x Fin x B\n","        x = x.permute(3,1,2,0).contiguous()  # B x V x Fin x K\n","        x = x.view([B*V, Fin*K])             # B*V x Fin*K\n","\n","        # Compose linearly Fin features to get Fout features\n","        x = cl(x)                            # B*V x Fout\n","        x = x.view([B, V, Fout])             # B x V x Fout\n","\n","        return x\n","\n","\n","    # Max pooling of size p. Must be a power of 2.\n","    def graph_max_pool(self, x, p):\n","        if p > 1:\n","            x = x.permute(0,2,1).contiguous()  # x = B x F x V\n","            x = nn.MaxPool1d(p)(x)             # B x F x V/p\n","            x = x.permute(0,2,1).contiguous()  # x = B x V/p x F\n","            return x\n","        else:\n","            return x\n","\n","\n","    def forward(self, x_in, d, L):\n","        \n","        x = x_in#[:,:self.num_gene]\n","        x_nn = x_in#[:,self.num_gene:]\n","\n","        x = x.unsqueeze(2) # B x V x Fin=1\n","        x = self.graph_conv_cheby(x, self.cl1, L[0], self.CL1_F, self.CL1_K)\n","\n","        x = F.relu(x)\n","        x = self.graph_max_pool(x, self.poolsize)\n","\n","        # flatten()\n","        x = x.view(-1, self.FC1Fin)\n","\n","\n","        \n","        ##############################################\n","        ##                  GAE_re                  ##\n","        ##############################################\n","        x_reAdj = 0 #torch.stack([F.sigmoid(torch.mm(z_i, z_i.t())) for z_i in x_reAdj])\n","        \n","        ##############################################\n","        ##                  GAE                     ##\n","        ##############################################\n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        x_hidden_gae = x\n","\n","        x_decode_gae = self.fc2(x_hidden_gae)\n","#        x_decode_gae = F.relu(x_decode_gae)\n","        if self.FC2_F != 0:                \n","            x_decode_gae = F.relu(x_decode_gae)\n","            x_decode_gae  = nn.Dropout(d)(x_decode_gae)            \n","            x_decode_gae = self.fc3(x_decode_gae)            \n","\n","        \n","\n","\n","        ##############################################\n","        ##                  GCN//NN                  ##\n","        ##############################################\n","        \n","      \n","        # NN\n","        x_nn = self.nn_fc1(x_nn) # B x V\n","        x_nn = F.relu(x_nn)\n","        x_nn = self.nn_fc2(x_nn)\n","        x_nn = F.relu(x_nn)\n","\n","#        x_hidden_ae = x_nn\n","#        x_decode_ae = self.nn_fc3(x_hidden_ae)\n","#        x_decode_ae = F.relu(x_decode_ae)\n","#        x_decode_ae = self.nn_fc4(x_hidden_ae)\n","#        x_decode_ae = F.relu(x_decode_ae)       \n","\n","        # concatenate layer  \n","        x = torch.cat((x_hidden_gae, x_nn),1)        \n","        x = self.FC_sum2(x)\n","        x = F.log_softmax(x)        \n","\n","        return x_decode_gae, x_hidden_gae, x, x_reAdj\n","\n","\n","    def loss(self, y1, y_target1,y2, y_target2,l2_regularization):\n","     \n","        loss1 = nn.MSELoss()(y1, y_target1)\n","        loss2 = nn.NLLLoss()(y2,y_target2)            \n","        loss = 1 * loss1 + 1 * loss2 \n","\n","        l2_loss = 0.0\n","        for param in self.parameters():\n","            data = param* param\n","            l2_loss += data.sum()\n","\n","\n","        loss += 0.2* l2_regularization* l2_loss\n","\n","        return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tCYwVwXCD1-S"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YNAmiLzYD2BC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UMTUKxjjD2Dn"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B2q5AUKKD2Gg"},"source":[""],"execution_count":null,"outputs":[]}]}